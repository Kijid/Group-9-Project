{
  "cells": [
    {
      "cell_type": "code",
      "source": "import praw\nimport pandas as pd\n\nimport glob\nfrom textblob import TextBlob\nimport csv",
      "metadata": {
        "source_hash": null,
        "execution_start": 1703860828470,
        "execution_millis": 8,
        "deepnote_to_be_reexecuted": false,
        "cell_id": "0e8a569ee0434e3da3bb35ce7f77b4db",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null,
      "block_group": "a5ce9313c9274d5dbfa3db08f1c158f0"
    },
    {
      "cell_type": "markdown",
      "source": "This code fetches data from various Reddit posts and stores them into pandas DataFrame. The primary steps involved are:\n\n- Establish a connection to the Reddit API using the PRAW library and the provided credentials.\n- Define a function called `fetch_subreddit_data()`. This function accepts a subreddit name and a list of post URLs. For each post, it fetches the title, author, content, score, and creation time, and appends them to a DataFrame. The function also fetches the similar data for each comment in a post if it is not deleted. It uses the `praw.models.Comment` to ensure the replies are indeed comments.\n- Defines a list of subreddit names and a dictionary of post URLs for each subreddit.\n- Iterates over the subreddit names, fetches data using the `fetch_subreddit_data()` function, and saves the output DataFrame to a CSV file named after the subreddit. The `index=False` argument in `to_csv` function ensures that the indexes are not included in the output CSV file.",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "38de40b83bb74042af2727aed3366f73",
        "deepnote_cell_type": "text-cell-p"
      },
      "block_group": "71ceeb4ec02a496f9b2cb5014cd84886"
    },
    {
      "cell_type": "code",
      "source": "# Setting up PRAW with our Reddit app credentials\nreddit = praw.Reddit(\n    client_id='1Q7zYJ3FBAowuOvu_axlLA',        \n    client_secret='OZXwSMsECF-1ZhIvVLE98gjrVOPdXw', \n    user_agent='MoT-Group9',      \n)",
      "metadata": {
        "source_hash": null,
        "execution_start": 1703860830139,
        "execution_millis": 145,
        "deepnote_to_be_reexecuted": false,
        "cell_id": "02c848cf9e1e4084bd99b1ad3e632641",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null,
      "block_group": "e5fa40fe4be641a6ae54090dc65ab6b3"
    },
    {
      "cell_type": "markdown",
      "source": "### Functions necessary to retrieve data from pre-selected sub-reddit posts",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "544809e06b284606886e21f993ee8c68",
        "deepnote_cell_type": "text-cell-h3"
      },
      "block_group": "39be464a661f44258949ef702d481415"
    },
    {
      "cell_type": "code",
      "source": "#this funciton will determine the sentiment of selected posts within the selected subreddits\ndef get_sentiment(text):\n    analysis = TextBlob(text)\n    # Classify the sentiment as positive, negative, or neutral\n    if analysis.sentiment.polarity > 0:\n        return 'positive'\n    elif analysis.sentiment.polarity < 0:\n        return 'negative'\n    else:\n        return 'neutral'",
      "metadata": {
        "source_hash": null,
        "execution_start": 1703860834847,
        "execution_millis": 10,
        "deepnote_to_be_reexecuted": false,
        "cell_id": "e107ebbd5729486c87e2b2f36cb0bacf",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null,
      "block_group": "5c70afd904dd4d57a1162b92dbc15270"
    },
    {
      "cell_type": "code",
      "source": "def fetch_comments(submission, subreddit_name):\n    submission.comments.replace_more(limit=None)\n    comments_data = pd.DataFrame(columns=['subreddit', 'post_title', 'post_sentiment', 'comment_id', 'parent_id', 'comment_author', 'comment_body', 'comment_score', 'comment_created_utc'])\n\n    for comment in submission.comments.list():\n        if isinstance(comment, praw.models.Comment) and comment.body != '[deleted]':\n            # Appending information about each comment to the DataFrame\n            comments_data = comments_data.append({\n                'subreddit': subreddit_name,\n                'post_title': submission.title,\n                'post_sentiment': get_sentiment(submission.title),\n                'comment_id': comment.id,\n                'parent_id': comment.parent_id,\n                'comment_author': comment.author.name if comment.author else '[deleted]',\n                'comment_body': comment.body,\n                'comment_score': comment.score,\n                'comment_created_utc': pd.to_datetime(comment.created_utc, unit='s')\n            }, ignore_index=True)\n\n    return comments_data",
      "metadata": {
        "source_hash": null,
        "execution_start": 1703860906650,
        "execution_millis": 9,
        "deepnote_to_be_reexecuted": false,
        "cell_id": "f70a6a42184249eaa3fec34ee2354ba5",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null,
      "block_group": "3bbb6ae8880a4af7bceaafafad131025"
    },
    {
      "cell_type": "code",
      "source": "# Creating a function to fetch subreddit data - takes in subreddit name and list of post URLs provided below.\ndef fetch_subreddit_data(subreddit_name, post_urls):\n    columns = ['subreddit', 'post_title', 'post_sentiment', 'comment_id', 'parent_id', 'comment_author', 'comment_body', 'comment_score', 'comment_created_utc']\n    comments_data = pd.DataFrame(columns=columns)\n\n    # Looping through the provided post URLs\n    for post_url in post_urls:\n        submission = reddit.submission(url=post_url)\n\n        # Appending information about the post to the DataFrame\n        comments_data = comments_data.append({\n            'subreddit': subreddit_name,\n            'post_title': submission.title,\n            'post_sentiment': get_sentiment(submission.title),\n            'comment_id': submission.id,\n            'parent_id': None,\n            'comment_author': submission.author.name if submission.author else '[deleted]',\n            'comment_body': submission.selftext,\n            'comment_score': submission.score,\n            'comment_created_utc': pd.to_datetime(submission.created_utc, unit='s')\n        }, ignore_index=True)\n\n        # Process comments and their replies\n        comments_data = comments_data.append(fetch_comments(submission, subreddit_name), ignore_index=True)\n\n    return comments_data",
      "metadata": {
        "source_hash": null,
        "execution_start": 1703857148350,
        "execution_millis": 9,
        "deepnote_to_be_reexecuted": false,
        "cell_id": "eb7deb31d8e246d19ee356423a1acefa",
        "deepnote_cell_type": "code"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "                subreddit                                         post_title  \\\n0   ArtificialInteligence  What is expected to gain from an AI safety sum...   \n1   ArtificialInteligence  What is expected to gain from an AI safety sum...   \n2   ArtificialInteligence  What is expected to gain from an AI safety sum...   \n3   ArtificialInteligence  What is expected to gain from an AI safety sum...   \n4   ArtificialInteligence  What is expected to gain from an AI safety sum...   \n5   ArtificialInteligence  What is expected to gain from an AI safety sum...   \n6   ArtificialInteligence  What is expected to gain from an AI safety sum...   \n7   ArtificialInteligence  What is expected to gain from an AI safety sum...   \n8   ArtificialInteligence  What is expected to gain from an AI safety sum...   \n9   ArtificialInteligence  What is expected to gain from an AI safety sum...   \n10  ArtificialInteligence  What is expected to gain from an AI safety sum...   \n11  ArtificialInteligence  What is expected to gain from an AI safety sum...   \n12  ArtificialInteligence  What is expected to gain from an AI safety sum...   \n13  ArtificialInteligence  What is expected to gain from an AI safety sum...   \n14  ArtificialInteligence  What is expected to gain from an AI safety sum...   \n15  ArtificialInteligence  What is expected to gain from an AI safety sum...   \n16  ArtificialInteligence  What is expected to gain from an AI safety sum...   \n17  ArtificialInteligence  What is expected to gain from an AI safety sum...   \n18  ArtificialInteligence  What is expected to gain from an AI safety sum...   \n19  ArtificialInteligence  What is expected to gain from an AI safety sum...   \n20  ArtificialInteligence  What is expected to gain from an AI safety sum...   \n21  ArtificialInteligence  What is expected to gain from an AI safety sum...   \n22  ArtificialInteligence  What is expected to gain from an AI safety sum...   \n23  ArtificialInteligence  What is expected to gain from an AI safety sum...   \n24  ArtificialInteligence  What is expected to gain from an AI safety sum...   \n25  ArtificialInteligence  What is expected to gain from an AI safety sum...   \n26  ArtificialInteligence  What is expected to gain from an AI safety sum...   \n27  ArtificialInteligence  What is expected to gain from an AI safety sum...   \n28  ArtificialInteligence  What is expected to gain from an AI safety sum...   \n29  ArtificialInteligence  What is expected to gain from an AI safety sum...   \n30  ArtificialInteligence  What is expected to gain from an AI safety sum...   \n31  ArtificialInteligence  What is expected to gain from an AI safety sum...   \n32  ArtificialInteligence  What is expected to gain from an AI safety sum...   \n33  ArtificialInteligence  What is expected to gain from an AI safety sum...   \n34  ArtificialInteligence  What is expected to gain from an AI safety sum...   \n35  ArtificialInteligence  What is expected to gain from an AI safety sum...   \n36  ArtificialInteligence  What is expected to gain from an AI safety sum...   \n37  ArtificialInteligence  What is expected to gain from an AI safety sum...   \n38  ArtificialInteligence  What is expected to gain from an AI safety sum...   \n39  ArtificialInteligence  What is expected to gain from an AI safety sum...   \n40  ArtificialInteligence  What is expected to gain from an AI safety sum...   \n41  ArtificialInteligence  What is expected to gain from an AI safety sum...   \n42  ArtificialInteligence  What is expected to gain from an AI safety sum...   \n43  ArtificialInteligence  What is expected to gain from an AI safety sum...   \n44  ArtificialInteligence  What is expected to gain from an AI safety sum...   \n45  ArtificialInteligence  What is expected to gain from an AI safety sum...   \n46  ArtificialInteligence  What is expected to gain from an AI safety sum...   \n47  ArtificialInteligence  What is expected to gain from an AI safety sum...   \n48  ArtificialInteligence  What is expected to gain from an AI safety sum...   \n49  ArtificialInteligence  What is expected to gain from an AI safety sum...   \n50  ArtificialInteligence  What is expected to gain from an AI safety sum...   \n51  ArtificialInteligence  What is expected to gain from an AI safety sum...   \n52  ArtificialInteligence  What is expected to gain from an AI safety sum...   \n53  ArtificialInteligence  What is expected to gain from an AI safety sum...   \n54  ArtificialInteligence  What is expected to gain from an AI safety sum...   \n55  ArtificialInteligence  What is expected to gain from an AI safety sum...   \n56  ArtificialInteligence  What is expected to gain from an AI safety sum...   \n57  ArtificialInteligence  What is expected to gain from an AI safety sum...   \n58  ArtificialInteligence  What is expected to gain from an AI safety sum...   \n\n   post_sentiment comment_id   parent_id        comment_author  \\\n0        positive    17ddumf        None             Xtianus21   \n1        positive    k5w1tv7  t3_17ddumf         AutoModerator   \n2        positive    k5wessn  t3_17ddumf        wellididntdoit   \n3        positive    k5xy993  t3_17ddumf      Super_Pole_Jitsu   \n4        positive    k5w8nx1  t3_17ddumf             zero-evil   \n5        positive    k5yinun  t3_17ddumf           No-Grab3052   \n6        positive    k5xol1k  t3_17ddumf  Original_Finding2212   \n7        positive    k5xyhwp  t3_17ddumf             [deleted]   \n8        positive    k5yqf3t  t3_17ddumf              Jdonavan   \n9        positive    k5z41kj  t1_k5xy993             Xtianus21   \n10       positive    k5xahca  t1_k5wlj5v             Xtianus21   \n11       positive    k5ytd8h  t1_k5ymwez             Xtianus21   \n12       positive    k5xoytx  t1_k5xol1k             Xtianus21   \n13       positive    k5ytj1n  t1_k5xyhwp             Xtianus21   \n14       positive    k5yrabb  t1_k5yqf3t             Xtianus21   \n15       positive    k5znqzn  t1_k5z41kj      Super_Pole_Jitsu   \n16       positive    k5xpglk  t1_k5xoytx  Original_Finding2212   \n17       positive    k5xyzlx  t1_k5xoytx             [deleted]   \n18       positive    k5ysiby  t1_k5yrabb              Jdonavan   \n19       positive    k612wm8  t1_k5znqzn             Xtianus21   \n20       positive    k5xq410  t1_k5xpglk             Xtianus21   \n21       positive    k5yrz22  t1_k5xyzlx             Xtianus21   \n22       positive    k5zgsm9  t1_k5ysiby             Xtianus21   \n23       positive    k61kvnj  t1_k612wm8      Super_Pole_Jitsu   \n24       positive    k5xqkrr  t1_k5xq410  Original_Finding2212   \n25       positive    k5y4bxd  t1_k5xq410            PewPewDiie   \n26       positive    k5yvc48  t1_k5yrz22             [deleted]   \n27       positive    k5zhqdk  t1_k5zgsm9              Jdonavan   \n28       positive    k61sqo7  t1_k61kvnj             Xtianus21   \n29       positive    k5xqykg  t1_k5xqkrr             Xtianus21   \n30       positive    k5yt3f2  t1_k5y4bxd             Xtianus21   \n31       positive    k5z5ed0  t1_k5yvc48             Xtianus21   \n32       positive    k60w6pd  t1_k5zhqdk             Xtianus21   \n33       positive    k61syp5  t1_k61sqo7      Super_Pole_Jitsu   \n34       positive    k5xr984  t1_k5xqykg  Original_Finding2212   \n35       positive    k5zgg6y  t1_k5z5ed0             [deleted]   \n36       positive    k61t8hd  t1_k61syp5             Xtianus21   \n37       positive    k5xrd9h  t1_k5xr984             Xtianus21   \n38       positive    k5zjb0e  t1_k5zgg6y             Xtianus21   \n39       positive    k61wwmp  t1_k61t8hd      Super_Pole_Jitsu   \n40       positive    k5xrnak  t1_k5xrd9h  Original_Finding2212   \n41       positive    k5zp7es  t1_k5zjb0e             [deleted]   \n42       positive    k62388n  t1_k61wwmp             Xtianus21   \n43       positive    k5xsdjw  t1_k5xrnak             Xtianus21   \n44       positive    k616c0q  t1_k5zp7es             Xtianus21   \n45       positive    k62zyci  t1_k62388n      Super_Pole_Jitsu   \n46       positive    k5xsh62  t1_k5xsdjw  Original_Finding2212   \n47       positive    k62jsp1  t1_k616c0q             [deleted]   \n48       positive    k63hkfy  t1_k62zyci             Xtianus21   \n49       positive    k5xsnzj  t1_k5xsh62             Xtianus21   \n50       positive    k5xtr2z  t1_k5xsh62             Xtianus21   \n51       positive    k62pfo7  t1_k62jsp1             Xtianus21   \n52       positive    k5y0bps  t1_k5xsnzj  Original_Finding2212   \n53       positive    k62u0zv  t1_k62pfo7             [deleted]   \n54       positive    k63hc6u  t1_k62u0zv             Xtianus21   \n55       positive    k63ne27  t1_k63hc6u             [deleted]   \n56       positive    k63ppph  t1_k63ne27             Xtianus21   \n57       positive    k63qz9l  t1_k63ppph             [deleted]   \n58       positive    k63vcmd  t1_k63qz9l             Xtianus21   \n\n                                         comment_body comment_score  \\\n0   For context, the [AI safety summit](https://ww...             0   \n1   ## Welcome to the r/ArtificialIntelligence gat...             1   \n2   AI just now is like computing in the 60's, eve...             6   \n3   Rarely do I see such a bad take.\\nNobody is cl...             5   \n4   There is nothing to fear from what is hyped as...             1   \n5   Ai side hustles to earn $10,000 :  https://med...             0   \n6   Any news about an AI rights movement about the...             1   \n7   It's like a survey\\n\\n\"Reply to this survey if...             1   \n8   I didn't read past your title.  If you want to...             1   \n9   Your take is well thought out but you're swing...             1   \n10                                               Huh?             0   \n11  I appreciate that. This is my goal. Let's all ...             1   \n12                         Do you think AI can think?             1   \n13  Do you think this? I think we are all just try...             1   \n14  What did I say that wasn't serious? BTW the ti...             1   \n15  I'm saying the math trick can get so good it's...             2   \n16  Really depends how you define “think”.\\n\\nI kn...             1   \n17  It can think as well as some humans. And produ...             1   \n18  The headline reads like a conspiracy rant. The...             2   \n19  Ok I see you're on the side of science here so...             1   \n20  Think like a sentient being. That's wgst I mea...             2   \n21  I'm not sure what you mean with the first sent...             1   \n22  Ahh I got you. No the tldr is about fear monge...             1   \n23  I have no idea what \"true intelligence\" means,...             1   \n24  I agree with all that, but it doesn’t invalida...             1   \n25  Well, does it really matter if it further down...             1   \n26  Humans are doing the same by that line of reas...             1   \n27  Here’s the thing.  Nobody with knowledge about...             1   \n28  Do you think GPT is aware of itself?  \\n\\n>Not...             1   \n29  No this abstraction of AI cannot gain consciou...             1   \n30  You give an interesting take. So you're saying...             1   \n31  I'm having trouble figuring out if you're or d...             1   \n32  Nobody is saying don't be worried about and it...             1   \n33  Of course it is. It knows a lot of things abou...             1   \n34  You are changing my words completely 🤔\\n\\nI do...             1   \n35  I'm disagreeing in that AI can think. Go quiz ...             1   \n36  So you think it is conscious? \\n\\nGPT prompt >...             1   \n37  You said semi-consciousness. There is no such ...             1   \n38  I do have a very good argument of why GPT can'...             0   \n39  >> who are you\\nI'm ChatGPT, a machine learnin...             1   \n40  I’m saying there is room to define what “semi-...             1   \n41  >You see GPT tells you it literally doesn't th...             1   \n42  >It's worth mentioning that openai heavily ste...             1   \n43  I'll say these 3 things. \\n\\n1. I'm saying tha...             2   \n44  Lol come on you're not serious are you. In fac...             0   \n45  I didn't say I thought it was conscious. I jus...             1   \n46  I agree with all of the above to the point. \\n...             1   \n47  Your \"un-altered prompt\" is the hidden system ...             1   \n48  No it doesn't claim that. You're making it up....             1   \n49  My concern with what you're saying, and by no ...             2   \n50  One last thing because I've enjoyed this conve...             2   \n51  I don't know how LLMs work? I work with LLMs a...             1   \n52  Your concern is accepted - this is not my inte...             1   \n53  If you ask a five year old how their brain wor...             0   \n54  What are you talking about. You can't be serio...             0   \n55  >I'm a data scentust and engineer.\\n\\nIf you'r...             0   \n56  No you're a person that believes artificial in...             0   \n57  >that believes artificial intelligence is aliv...             0   \n58  Do you know what a query is? In data science? ...             0   \n\n   comment_created_utc  \n0  2023-10-21 22:09:01  \n1  2023-10-21 22:09:02  \n2  2023-10-21 23:43:25  \n3  2023-10-22 08:27:15  \n4  2023-10-21 22:58:23  \n5  2023-10-22 12:34:31  \n6  2023-10-22 06:21:16  \n7  2023-10-22 08:30:31  \n8  2023-10-22 13:41:00  \n9  2023-10-22 15:19:46  \n10 2023-10-22 03:50:51  \n11 2023-10-22 14:03:47  \n12 2023-10-22 06:26:07  \n13 2023-10-22 14:05:00  \n14 2023-10-22 13:47:47  \n15 2023-10-22 17:27:43  \n16 2023-10-22 06:32:25  \n17 2023-10-22 08:37:05  \n18 2023-10-22 13:57:12  \n19 2023-10-22 22:43:39  \n20 2023-10-22 06:40:42  \n21 2023-10-22 13:53:05  \n22 2023-10-22 16:44:03  \n23 2023-10-23 00:52:16  \n24 2023-10-22 06:46:42  \n25 2023-10-22 09:48:14  \n26 2023-10-22 14:18:21  \n27 2023-10-22 16:50:07  \n28 2023-10-23 01:48:21  \n29 2023-10-22 06:51:32  \n30 2023-10-22 14:01:43  \n31 2023-10-22 15:28:54  \n32 2023-10-22 21:57:27  \n33 2023-10-23 01:49:58  \n34 2023-10-22 06:55:19  \n35 2023-10-22 16:41:50  \n36 2023-10-23 01:51:58  \n37 2023-10-22 06:56:44  \n38 2023-10-22 17:00:10  \n39 2023-10-23 02:19:00  \n40 2023-10-22 07:00:21  \n41 2023-10-22 17:36:32  \n42 2023-10-23 03:05:04  \n43 2023-10-22 07:09:47  \n44 2023-10-22 23:07:22  \n45 2023-10-23 09:20:58  \n46 2023-10-22 07:11:03  \n47 2023-10-23 05:45:15  \n48 2023-10-23 12:31:48  \n49 2023-10-22 07:13:29  \n50 2023-10-22 07:27:42  \n51 2023-10-23 06:55:31  \n52 2023-10-22 08:54:34  \n53 2023-10-23 07:58:14  \n54 2023-10-23 12:29:50  \n55 2023-10-23 13:18:18  \n56 2023-10-23 13:35:32  \n57 2023-10-23 13:44:40  \n58 2023-10-23 14:14:57  \n",
          "output_type": "stream"
        }
      ],
      "execution_count": null,
      "block_group": "ad8dc701c2944885ba9d15e0af08b855"
    },
    {
      "cell_type": "markdown",
      "source": "### Applying the functions to extract data ",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "318a9cb6d7d9440b97bb332d5455f710",
        "deepnote_cell_type": "text-cell-h3"
      },
      "block_group": "e1358caf42484fc2bc539083cdcf602e"
    },
    {
      "cell_type": "code",
      "source": "# Example usage:\nsubreddit_names = ['singularity', 'Futurology', 'ArtificialInteligence']\npost_urls_for_each_subreddit = {\n    'singularity': ['https://www.reddit.com/r/singularity/comments/132kgur/i_dont_fear_malicious_or_rogue_ai_i_fear_how_ai/', 'https://www.reddit.com/r/singularity/comments/12983il/the_reason_i_dont_fear_artificial_intelligence/'],\n    'Futurology': ['https://www.reddit.com/r/Futurology/comments/9z9g0j/why_do_we_fear_artificial_intelligence_andor/'],\n    'ArtificialInteligence': ['https://www.reddit.com/r/ArtificialInteligence/comments/17ddumf/what_is_expected_to_gain_from_an_ai_safety_summit/']\n}\n\ngithub_linked_folder = '/work/GitHub_ML_Deepnote/Machine Learning/1. Extracted Reddit Data/'",
      "metadata": {
        "source_hash": null,
        "execution_start": 1703854457128,
        "execution_millis": 11,
        "deepnote_to_be_reexecuted": false,
        "cell_id": "a83feb1b89fa4f6eab4bc8fb673c758d",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null,
      "block_group": "6cc327908509482dad44e5c0bb86b1c7"
    },
    {
      "cell_type": "code",
      "source": "# Creating cvs files categorising data by subreddit\nfor subreddit_name in subreddit_names:\n    comments_data = fetch_subreddit_data(subreddit_name, post_urls_for_each_subreddit[subreddit_name])\n\n    # Specify the path for saving CSV files within the GitHub-linked folder\n    csv_file_path = f'{github_linked_folder}{subreddit_name}_comments_data.csv'\n\n    # Save the CSV file\n    comments_data.to_csv(csv_file_path, index=False, encoding='utf-8')\n\n",
      "metadata": {
        "source_hash": null,
        "execution_start": 1703854457129,
        "execution_millis": 4771,
        "deepnote_to_be_reexecuted": false,
        "cell_id": "7264ad9169ee4e7b9c4e52f3b78820e5",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null,
      "block_group": "45a511b0cf27454f84ff34a7605a7d7c"
    },
    {
      "cell_type": "code",
      "source": "#Creating a csv file with containing data for all subreddits\nall_dataframes = []\n\nfor subreddit_name in subreddit_names:\n    comments_data = fetch_subreddit_data(subreddit_name, post_urls_for_each_subreddit[subreddit_name])\n    all_dataframes.append(comments_data)\n\n    # Specify the path for saving CSV files within the GitHub-linked folder\n    csv_file_path = f'{github_linked_folder}{subreddit_name}_comments_data.csv'\n\n    # Save the CSV file\n    comments_data.to_csv(csv_file_path, index=False, encoding='utf-8')\n\n# Concatenate all DataFrames into one\nall_comments_data = pd.concat(all_dataframes, ignore_index=True)\n\n# Specify the path for saving the combined CSV file\ncombined_csv_file_path = f'{github_linked_folder}>all_comments.csv'\n\n# Save the combined CSV file\nall_comments_data.to_csv(combined_csv_file_path, index=False, encoding='utf-8')",
      "metadata": {
        "source_hash": null,
        "execution_start": 1703854461901,
        "execution_millis": 4557,
        "deepnote_to_be_reexecuted": false,
        "cell_id": "10ab1f1b8de041bb95fb7b4380a43ec0",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null,
      "block_group": "75b5fea026dc44838d8d858ec39eb9ab"
    },
    {
      "cell_type": "markdown",
      "source": "Will likely want to get rid of comment_authors and use comments ids instead",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "4a70b05beb22438ebb1f1addd2ea2a98",
        "deepnote_cell_type": "text-cell-p"
      },
      "block_group": "b599429acd7046a39b5f589542170a44"
    },
    {
      "cell_type": "markdown",
      "source": "### Creating a summary of subreddit posts",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "15aff740b7644fd6ae236535349f0ba2",
        "deepnote_cell_type": "text-cell-h3"
      },
      "block_group": "f33c1b908d2b44a2895ecf570ed597eb"
    },
    {
      "cell_type": "code",
      "source": "# Path to the folder containing all data for subreddit comments as CSV files\nfolder_path = '/work/GitHub_ML_Deepnote/Machine Learning/1. Extracted Reddit Data/'\n\n# Retrieving all CSV files in the folder\ncsv_files = glob.glob(f'{folder_path}*comments_data.csv')\n\n# Initializing an empty list to store DataFrames\nall_dataframes = []\n\n# Iterating through each CSV file and reading it into a DataFrame\nfor csv_file in csv_files:\n    df = pd.read_csv(csv_file)\n    all_dataframes.append(df)\n\n# Concatenating all DataFrames into one\nall_summary_data = pd.concat(all_dataframes, ignore_index=True)\n\n# Create a summary DataFrame with the total number of comments for each post\nsummary_data = all_summary_data.groupby(['subreddit', 'post_title', 'post_sentiment']).size().reset_index(name='total_comments')\n\n# Add the 'url' column based on the predefined URLs\nsummary_data['url'] = summary_data['subreddit'].map(post_urls_for_each_subreddit)\n\n# Add a 'post_id' column using the index as a unique identifier\nsummary_data['post_id'] = summary_data.index.map(lambda x: f'{x+1:03d}')\n\n# Rearrange the order of columns\nsummary_data = summary_data[['subreddit', 'post_id', 'post_sentiment','post_title', 'total_comments', 'url']]\n\n# Specifying the path for saving the overall summary CSV file\noverall_summary_csv_file_path = '/work/GitHub_ML_Deepnote/Machine Learning/1. Extracted Reddit Data/>subreddits_summary.csv'\n\n# Save the overall summary CSV file\nsummary_data.to_csv(overall_summary_csv_file_path, index=False)\n\n# Display the overall summary data\nprint(summary_data)",
      "metadata": {
        "source_hash": null,
        "execution_start": 1703854466456,
        "execution_millis": 2077,
        "deepnote_to_be_reexecuted": false,
        "cell_id": "0a066f72d63647019faf3e1485f7f5b3",
        "deepnote_cell_type": "code"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "               subreddit post_id post_sentiment  \\\n0  ArtificialInteligence     001       positive   \n1             Futurology     002       negative   \n2            singularity     003       negative   \n3            singularity     004       negative   \n\n                                          post_title  total_comments  \\\n0  What is expected to gain from an AI safety sum...              59   \n1  Why do we fear artificial intelligence and/or ...              42   \n2  I don't fear malicious or rogue AI - I fear ho...             153   \n3    The reason I don’t fear artificial intelligence              32   \n\n                                                 url  \n0  [https://www.reddit.com/r/ArtificialInteligenc...  \n1  [https://www.reddit.com/r/Futurology/comments/...  \n2  [https://www.reddit.com/r/singularity/comments...  \n3  [https://www.reddit.com/r/singularity/comments...  \n",
          "output_type": "stream"
        }
      ],
      "execution_count": null,
      "block_group": "da7aedcba4304453a2aa87b0abd8d190"
    },
    {
      "cell_type": "markdown",
      "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=f64215d6-debc-46bd-b273-63565459a66d' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
      "metadata": {
        "created_in_deepnote_cell": true,
        "deepnote_cell_type": "markdown"
      }
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "deepnote": {},
    "orig_nbformat": 2,
    "deepnote_notebook_id": "668d44b62c5e4f4f952579ee125a1372",
    "deepnote_persisted_session": {
      "createdAt": "2023-12-29T15:45:26.124Z"
    },
    "deepnote_execution_queue": []
  }
}