{
  "cells": [
    {
      "cell_type": "code",
      "source": "# Packages needed\n\nimport pandas as pd\nimport numpy as np\nimport praw\nimport nltk\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nimport string\nimport re\n\n",
      "metadata": {
        "source_hash": null,
        "execution_start": 1703854386384,
        "execution_millis": 513,
        "deepnote_to_be_reexecuted": false,
        "cell_id": "e7f4d90c6f7946d8bccead7a9f22beed",
        "deepnote_cell_type": "code"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'praw'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [1], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpraw\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstem\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordNetLemmatizer\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'praw'"
          ]
        }
      ],
      "execution_count": null,
      "block_group": "eca6572fca504fd881caf04a58174528"
    },
    {
      "cell_type": "markdown",
      "source": "### Aliya 's code from section 2 (30/12/23) ",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "ffc71161884b41abb5924ec3b69926c2",
        "deepnote_cell_type": "text-cell-h3"
      },
      "block_group": "1c68ec8b545747ce878713e4e6e9388c"
    },
    {
      "cell_type": "code",
      "source": "import pandas as pd\nimport nltk\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nimport string\nimport re\n\nnltk.download('punkt')\nnltk.download('stopwords')\n\nall_comments.to_csv('/work/GitHub_ML_Deepnote/Machine Learning/1. Extracted Reddit Data/>all_comments.csv', encoding='utf-8')\n\nlemmatiser = WordNetLemmatizer()\nstopwords_set = set(stopwords.words('english'))\npunctuation_set = set(string.punctuation)\n\nurl_pattern = re.compile(r\"http\\S+|www\\S+\")\nusername_pattern = re.compile(r\"\\B/u/\\w+\\b\")\n\ndef preprocess_comment(comment_text):\n    clean_comment = url_pattern.sub(\"\", comment_text)\n    clean_comment = username_pattern.sub(\"\", clean_comment)\n\n    tokens = word_tokenize(clean_comment)\n    lemmatised_tokens = [lemmatiser.lemmatize(token.lower()) for token in tokens if token.lower() not in stopwords_set and token not in punctuation_set and token.isalnum()]\n    lemmatised_comment = ' '.join(lemmatised_tokens)\n\n    return lemmatised_comment\n\nall_comments['processed_comment'] = all_comments['comment_body'].apply(preprocess_comment)\n",
      "metadata": {
        "cell_id": "2273ba24cb6c4feaaa152dd430c78c28",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null,
      "block_group": "7a7b7e8914974188976f43ed6c2c2efc"
    },
    {
      "cell_type": "code",
      "source": "nlp= spacy.load('en_core_web_sm')\n\ndef extract_entities(comment_text):\n    doc = nlp(comment_text)\n    entities = [(ent.text, ent.label_) for ent in doc.ents]\n    return entities\n\nall_comments['entities']= all_comments['processed_comment'].apply(extract_entities)\n\n# Print the first few processed comments and their entities for review\nprint(all_comments[['processed_comment', 'entities']].head())",
      "metadata": {
        "cell_id": "46b88772619d4a31810f972ee6c394f2",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null,
      "block_group": "05214b6730064d90b9aeb8d49579a435"
    },
    {
      "cell_type": "code",
      "source": "all_comments['processed_comment'].iloc[0]",
      "metadata": {
        "cell_id": "fe1dea63efab4275b04a01d6e2837adc",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null,
      "block_group": "57b2ee2c43534e4b818e3e053f6c8eb9"
    },
    {
      "cell_type": "markdown",
      "source": "### Aliya continued - Version 2 ",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "8c455c5dbe2146aabadfcb4c50e183f8",
        "deepnote_cell_type": "text-cell-h3"
      },
      "block_group": "d98b94dfa45a4fa79379fb9bf6be656c"
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "cell_id": "4202550a3dfc41d1ab7cfb78348b6b76",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null,
      "block_group": "f4a93837ded849699b153e0ccb00f34e"
    },
    {
      "cell_type": "code",
      "source": "# Need to create a corpus - then csv file\n\n\n#Aliya: code to extract data from Reddit  - \n\nnltk.download('punkt') \nnltk.download('stopwords')  \n\n# Reddit API authentication\nreddit = praw.Reddit(\n    client_id='',\n    client_secret='',\n    username='',\n    password='',\n    user_agent='Summative'\n)\nsubreddit = reddit.subreddit('singularity)\ncomments = subreddit.comments(limit=1000) #we can change the limit - are we looking at the comments of each subreddit?\n\n# If looking at specific posts\npost_id = '11xwuqp' #ID from the URL https://www.reddit.com/r/singularity/comments/11xwuqp/why_is_no_one_talking_about_ai/\npost_1 = reddit.submission(id=post_id)\npost_1.comments.replace_more(limit=None)\n",
      "metadata": {
        "source_hash": null,
        "deepnote_to_be_reexecuted": true,
        "cell_id": "24e9b4b8a7a84ec79c998f8d22af9ff2",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null,
      "block_group": "24e9b4b8a7a84ec79c998f8d22af9ff2"
    },
    {
      "cell_type": "code",
      "source": "nlp= spacy.load('en_core_web_sm')\nurl_pattern= re.compile(r'http\\S+|www\\S+')\nusername_pattern= re.compile(r'\\B/u/\\w+\\b')\n\ndef preprocess_comment(comment_text):\n    clean_comment= url_pattern.sub('', comment_text)\n    clean_comment= username_pattern.sub('', clean_comment)\n\n    doc = nlp(clean_comment)\n\n    lemmatised_tokens= [\n        token.lemma_.lower()\n        for token in doc\n        if not token.is_stop and not token.is_punct and token.is_alpha\n    ]\n\n    lemmatised_comment= ' '.join(lemmatised_tokens)\n\n    return lemmatised_comment\n\nall_comments['processed_comment'] = all_comments['comment_body'].apply(preprocess_comment)\n\nall_comments.to_csv('/work/GitHub_ML_Deepnote/Machine Learning/1. Extracted Reddit Data/>all_comments.csv', encoding='utf-8')\n\n\nprint(all_comments['processed_comment'])",
      "metadata": {
        "source_hash": null,
        "deepnote_to_be_reexecuted": true,
        "cell_id": "22c638de3b85414cb658abca2e9671a7",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null,
      "block_group": "9f0bbf75c8e44fe798e4479388279e46"
    },
    {
      "cell_type": "code",
      "source": "sample_text = \"I'm thinking about the future of AI and machine learning.\"\n\nsample = preprocess_comment(sample_text)\nprint('Checking if lemmatised:', sample)\n",
      "metadata": {
        "source_hash": null,
        "deepnote_to_be_reexecuted": true,
        "cell_id": "8960068bc9bc41d59a3dcdb4f110de8e",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null,
      "block_group": "e3c8a99bd0cf4078b3edac8fdcec327f"
    },
    {
      "cell_type": "code",
      "source": "all_comments['processed_comment']= all_comments['comment_body'].apply(preprocess_comment)\n\nprint(all_comments[['comment_body', 'processed_comment']])",
      "metadata": {
        "cell_id": "4acfc2fbb9df4754950d3af43c58042d",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null,
      "block_group": "6035ca82b0454bc084207c7adceca2cd"
    },
    {
      "cell_type": "code",
      "source": "# Saving preprocessed comments Dataframe to CSV\nprocessed_comments= all_comments[['subreddit', 'post_title', 'post_sentiment', 'comment_id', 'parent_id', 'comment_author', 'comment_body', 'processed_comment', 'comment_score', 'comment_created_utc']]\n\nprocessed_comments_csv_path= '/work/GitHub_ML_Deepnote/Machine Learning/2. Cleaning & Pre-processing/processed_comments.csv'\n\nprocessed_comments.to_csv(processed_comments_csv_path, index=False, encoding='utf-8')\n\nprint(f'Processed comments data saved to: {processed_comments_csv_path}')",
      "metadata": {
        "cell_id": "7b151f4d67d24de3aa80c913b932b750",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null,
      "block_group": "ae1ab7be34e74a7eaeb3ae22aeda8456"
    },
    {
      "cell_type": "markdown",
      "source": "### DILAN Storage ",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "12122fe3591e4924b4560998eaf2df3b",
        "deepnote_cell_type": "text-cell-h3"
      },
      "block_group": "53d98048c6ca41ff98e69e02677a6b0f"
    },
    {
      "cell_type": "code",
      "source": "#ew i don't like spacy\nimport spacy\n\n# Load the spaCy English language model\nnlp = spacy.load(\"en_core_web_sm\")\n\n# Assuming 'cleaned_comment' is the column containing cleaned comments\nword_level_df = cleaned_comments.copy()\n\n# Lemmatization using spaCy, handling None values\nword_level_df['lemmatized_comments'] = word_level_df['cleaned_comment'].apply(lambda x: [token.lemma_ for token in nlp(x)] if x else [])\n\n# Display the Word-Level Analysis DataFrame\nprint(word_level_df[['cleaned_comment', 'lemmatized_comments']])",
      "metadata": {
        "cell_id": "9c045f206841481b9cb0a97c70c2b67d",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null,
      "block_group": "f4387bb7e2034eb6906e33bc25202958"
    },
    {
      "cell_type": "code",
      "source": "test_words = ['thinking', 'exploitation']\nlemmatized_test_words = [lemmatizer.lemmatize(word) for word in test_words]\nprint(lemmatized_test_words)",
      "metadata": {
        "cell_id": "4496d0da2f6d47ba954117b09b8cd72e",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null,
      "block_group": "f9be434125b5419b92d2f68e3735fbdc"
    },
    {
      "cell_type": "code",
      "source": "#Checking specific words and their corresponding lemmas as nouns vs verbs:\nfrom nltk.stem import WordNetLemmatizer\n\nlemmatizer = WordNetLemmatizer()\n\nword = \"thinking\"\npos_tag = 'v'  # 'v' for verb\n\nlemmatized_word = lemmatizer.lemmatize(word, pos_tag)\nprint(lemmatized_word)",
      "metadata": {
        "source_hash": "2c273c82",
        "execution_start": 1703964901223,
        "execution_millis": 4650,
        "deepnote_to_be_reexecuted": false,
        "cell_id": "f6488818db4b4e71a69ea810abc53cf7",
        "deepnote_cell_type": "code"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [1], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m word \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthinking\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m pos_tag \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mv\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# 'v' for verb\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m lemmatized_word \u001b[38;5;241m=\u001b[39m \u001b[43mlemmatizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlemmatize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_tag\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(lemmatized_word)\n",
            "File \u001b[0;32m~/venv/lib/python3.9/site-packages/nltk/stem/wordnet.py:45\u001b[0m, in \u001b[0;36mWordNetLemmatizer.lemmatize\u001b[0;34m(self, word, pos)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlemmatize\u001b[39m(\u001b[38;5;28mself\u001b[39m, word: \u001b[38;5;28mstr\u001b[39m, pos: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;124;03m\"\"\"Lemmatize `word` using WordNet's built-in morphy function.\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;124;03m    Returns the input word unchanged if it cannot be found in WordNet.\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;124;03m    :return: The lemma of `word`, for the given `pos`.\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m     lemmas \u001b[38;5;241m=\u001b[39m \u001b[43mwn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_morphy\u001b[49m(word, pos)\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(lemmas, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m lemmas \u001b[38;5;28;01melse\u001b[39;00m word\n",
            "File \u001b[0;32m~/venv/lib/python3.9/site-packages/nltk/corpus/util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
            "File \u001b[0;32m~/venv/lib/python3.9/site-packages/nltk/corpus/util.py:89\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[0;32m---> 89\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__reader_cls\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# This is where the magic happens!  Transform ourselves into\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# the corpus by modifying our own __dict__ and __class__ to\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# match that of the corpus.\u001b[39;00m\n\u001b[1;32m     95\u001b[0m args, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs\n",
            "File \u001b[0;32m~/venv/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:1206\u001b[0m, in \u001b[0;36mWordNetCorpusReader.__init__\u001b[0;34m(self, root, omw_reader)\u001b[0m\n\u001b[1;32m   1203\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lexnames\u001b[38;5;241m.\u001b[39mappend(lexname)\n\u001b[1;32m   1205\u001b[0m \u001b[38;5;66;03m# Load the indices for lemmas and synset offsets\u001b[39;00m\n\u001b[0;32m-> 1206\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_lemma_pos_offset_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1208\u001b[0m \u001b[38;5;66;03m# load the exception file data into memory\u001b[39;00m\n\u001b[1;32m   1209\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_exception_map()\n",
            "File \u001b[0;32m~/venv/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:1366\u001b[0m, in \u001b[0;36mWordNetCorpusReader._load_lemma_pos_offset_map\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1362\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m suffix \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_FILEMAP\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m   1363\u001b[0m \n\u001b[1;32m   1364\u001b[0m     \u001b[38;5;66;03m# parse each line of the file (ignoring comment lines)\u001b[39;00m\n\u001b[1;32m   1365\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex.\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m suffix) \u001b[38;5;28;01mas\u001b[39;00m fp:\n\u001b[0;32m-> 1366\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i, line \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(fp):\n\u001b[1;32m   1367\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m line\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1368\u001b[0m                 \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
            "File \u001b[0;32m~/venv/lib/python3.9/site-packages/nltk/data.py:1152\u001b[0m, in \u001b[0;36mSeekableUnicodeStreamReader.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1151\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/venv/lib/python3.9/site-packages/nltk/data.py:1145\u001b[0m, in \u001b[0;36mSeekableUnicodeStreamReader.next\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1144\u001b[0m     \u001b[38;5;124;03m\"\"\"Return the next decoded line from the underlying stream.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1145\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1146\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m line:\n\u001b[1;32m   1147\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m line\n",
            "File \u001b[0;32m~/venv/lib/python3.9/site-packages/nltk/data.py:1104\u001b[0m, in \u001b[0;36mSeekableUnicodeStreamReader.readline\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m   1100\u001b[0m new_chars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read(readsize)\n\u001b[1;32m   1102\u001b[0m \u001b[38;5;66;03m# If we're at a '\\r', then read one extra character, since\u001b[39;00m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# it might be a '\\n', to get the proper line ending.\u001b[39;00m\n\u001b[0;32m-> 1104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_chars \u001b[38;5;129;01mand\u001b[39;00m new_chars\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1105\u001b[0m     new_chars \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   1107\u001b[0m chars \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m new_chars\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "execution_count": 1,
      "block_group": "5fea71e0829c4cb79bb51d1e81fa6e57"
    },
    {
      "cell_type": "markdown",
      "source": "### VADER sentiment analysis",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "1a2bf9843a1b40daa96080f436ecf3fd",
        "deepnote_cell_type": "text-cell-h3"
      },
      "block_group": "86571e155681419880db04f502d3de45"
    },
    {
      "cell_type": "code",
      "source": "from nltk.sentiment.vader import SentimentIntensityAnalyzer\nimport nltk\nnltk.download('vader_lexicon')\n\n# Create a SentimentIntensityAnalyzer instance\nsia = SentimentIntensityAnalyzer()\n\n# Example text\ntext = \"I love the beautiful sunset on the beach. It makes me feel calm and happy.\"\n\n# Analyze the text and get sentiment scores\nsentiment_scores = sia.polarity_scores(text)\n\n# Print the sentiment scores\nprint(sentiment_scores)",
      "metadata": {
        "source_hash": null,
        "execution_start": 1703711479850,
        "execution_millis": 853,
        "deepnote_to_be_reexecuted": true,
        "cell_id": "1067dcafd9fe48a4942a8b114ff29d91",
        "deepnote_cell_type": "code"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "{'neg': 0.0, 'neu': 0.415, 'pos': 0.585, 'compound': 0.9337}\n[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n[nltk_data]   Package vader_lexicon is already up-to-date!\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null,
      "block_group": "13806178da224cdf8f72469205aedc27"
    },
    {
      "cell_type": "code",
      "source": "\n#DISFUNCTIONAL\n!pip install Emotion-recognition as emoreact\n\nfrom emotion_recognition import EmotionRecognizer\n\nsample_comment = \"I'm feeling really excited about the future of AI!\"\n\n# Initialize the emotion recognizer\nemorec = EmotionRecognizer()\n\n# Apply emotion recognition to the sample comment\nemotion_predictions = emorec.predict(sample_comment)\n\n# Display the results\nprint(f\"Sample Comment: {sample_comment}\")\nprint(\"Emotion Predictions:\", emotion_predictions)\n\n\n!pip install --upgrade pip\n!pip install emoreact\nfrom emoreact import EmoReact\n\n# Create an EmoReact instance\nemo_react = EmoReact()\n\n# Example text\ntext = \"I love the beautiful sunset on the beach. It makes me feel calm and happy.\"\n\n# Analyze the text and get VAD scores\nvad_scores = emo_react(text)\n\n# Print the VAD scores\nprint(vad_scores)",
      "metadata": {
        "source_hash": null,
        "execution_start": 1703711667334,
        "execution_millis": 2205,
        "deepnote_to_be_reexecuted": true,
        "cell_id": "b3f6dd18175740da8a4e6d78efc91b37",
        "deepnote_cell_type": "code"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Requirement already satisfied: Emotion-recognition in /root/venv/lib/python3.9/site-packages (1.0.2)\n\u001b[31mERROR: Could not find a version that satisfies the requirement as (from versions: none)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for as\u001b[0m\u001b[31m\n\u001b[0m",
          "output_type": "stream"
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'emotion_recognition'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip install Emotion-recognition as emoreact\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01memotion_recognition\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EmotionRecognizer\n\u001b[1;32m      5\u001b[0m sample_comment \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mm feeling really excited about the future of AI!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Initialize the emotion recognizer\u001b[39;00m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'emotion_recognition'"
          ]
        }
      ],
      "execution_count": null,
      "block_group": "ca8e65980dea49dbbb4a2e44ae8af52d"
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "source_hash": null,
        "deepnote_to_be_reexecuted": true,
        "cell_id": "95979aa5279d46d484425fc466dac281",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null,
      "block_group": "807403e05d7a41819e0860684c32e6c1"
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "source_hash": null,
        "deepnote_to_be_reexecuted": true,
        "cell_id": "4b609c0a4dad4455a8ef4a4db47230a9",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null,
      "block_group": "b2f56d43bebb486298fe1832090bb89a"
    },
    {
      "cell_type": "markdown",
      "source": "# Removed/ deleted code",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "9ccdf4f506864f688bce54ac6135e2b1",
        "deepnote_cell_type": "text-cell-h1"
      },
      "block_group": "20a1f900d9184ecc825a84abb3dd5cee"
    },
    {
      "cell_type": "markdown",
      "source": "### Cleaning and pre-processing",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "0fec0494473347bb8971dbe3cb4c9825",
        "deepnote_cell_type": "text-cell-h3"
      },
      "block_group": "e2cdb5221eb04eceb010108dc7953465"
    },
    {
      "cell_type": "code",
      "source": "#Function to clean comment body\ndef clean_reddit_comment(text):\n    # Removing links that might interfere with tokenization\n    text = re.sub(r'http\\S+', '', text)\n    # Removing usernames to reduce noise\n    text = re.sub(r'@[^\\s]+', '', text)\n    # Removing new lines\n    text = text.replace('\\n', ' ')\n    #Removing \"&#x200B;\" character\n    text = text.replace(\"&#x200B;\", \"\")\n    # Removing symbols or characters that might not contribute to the analysis.\n    text = re.sub(r'[^\\w\\s]', '', text)\n\n    # Check if text is empty after cleaning\n    if not text.strip():\n        return None\n    \n    return text",
      "metadata": {
        "cell_id": "6832a38b9bf045789c9eecb7e5b407da",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null,
      "block_group": "985c0a32916a4893bf7029013f2b1654"
    },
    {
      "cell_type": "code",
      "source": "#Function to clean comment body\ndef clean_reddit_comment(text):\n    # Removing links that might interfere with tokenization\n    text = re.sub(r'http\\S+', '', text)\n    # Removing usernames to reduce noise\n    text = re.sub(r'@[^\\s]+', '', text)\n    # Removing new lines\n    text = text.replace('\\n', ' ')\n    #Removing \"&#x200B;\" character\n    text = text.replace(\"&#x200B;\", \"\")\n    # Removing symbols or characters that might not contribute to the analysis.\n    text = re.sub(r'[^\\w\\s]', '', text)\n\n    # Check if text is empty after cleaning\n    if not text.strip():\n        return None\n    \n    return text",
      "metadata": {
        "cell_id": "1fd1143ca55c48cb8ad05931e8eef5ea",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null,
      "block_group": "eadabc85cc2a4be5ab5c77dc8b517ff9"
    },
    {
      "cell_type": "code",
      "source": "# Create a new DataFrame for cleaned comments\ncleaned_comments = all_comments[['subreddit', 'post_title', 'post_sentiment', 'comment_id', 'parent_id', 'comment_author', 'comment_body','cleaned_comment', 'comment_score', 'comment_created_utc']]\n\n# Specify the path for saving the cleaned comments CSV file\ncleaned_comments_csv_path = '/work/GitHub_ML_Deepnote/Machine Learning/2. Cleaning & Pre-processing/cleaned_comments.csv'\n\n# Save the cleaned comments DataFrame to a new CSV file\ncleaned_comments.to_csv(cleaned_comments_csv_path, index=False, encoding='utf-8')\n\n# Display the path to the saved CSV file\nprint(f\"Cleaned comments data saved to: {cleaned_comments_csv_path}\")",
      "metadata": {
        "cell_id": "f05dbe9095944930a1448bd93afcbf88",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null,
      "block_group": "3fb6fe7da3e248c48fa8c18fa2e493c9"
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "cell_id": "cc46cae7c654413db163460f7bebdfe2",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null,
      "block_group": "45cdfb3ea6fd4e03b5393a0a49f656af"
    },
    {
      "cell_type": "markdown",
      "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=f64215d6-debc-46bd-b273-63565459a66d' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
      "metadata": {
        "created_in_deepnote_cell": true,
        "deepnote_cell_type": "markdown"
      }
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "deepnote": {},
    "orig_nbformat": 2,
    "deepnote_notebook_id": "5eb3e6018276454d945a71025cb0d55b",
    "deepnote_persisted_session": {
      "createdAt": "2023-12-29T14:15:04.896Z"
    },
    "deepnote_execution_queue": []
  }
}