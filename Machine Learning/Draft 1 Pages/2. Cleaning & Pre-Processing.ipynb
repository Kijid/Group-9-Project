{"cells":[{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1703967356196,"execution_millis":181,"deepnote_to_be_reexecuted":false,"cell_id":"6772e92f46e042c9b678e4541459b769","deepnote_cell_type":"code"},"source":"import pandas as pd\nimport re\nimport string\n\n\n# NLTK for Natural Language Processing\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk import pos_tag\n\n# Downloading NLTK resources\nnltk.download('stopwords')\nnltk.download('punkt')\nnltk.download('wordnet')  \nnltk.download('omw-1.4')\nnltk.download('averaged_perceptron_tagger')\n\n# NLTK Pre-processing Setup\nstops = set(stopwords.words('english'))\nlemmatizer = WordNetLemmatizer()\npunct = set(string.punctuation)","block_group":"20d3cc45d95a4f25abd5aa9cfabb9e1a","execution_count":null,"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package stopwords to /root/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /root/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package wordnet to /root/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n[nltk_data]   Package omw-1.4 is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /root/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1703963271456,"execution_millis":986,"deepnote_to_be_reexecuted":false,"cell_id":"d7cc071230f542dd89f59d8e4c610512","deepnote_cell_type":"code"},"source":"# Load the comments data from the checkpoint CSV file\nall_comments = pd.read_csv('/work/GitHub_ML_Deepnote/Machine Learning/1. Extracted Reddit Data/>all_comments.csv', encoding='utf-8')","block_group":"173a60fb12094a08a56423db63729cc9","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"570e9d1edd11449fa53764fd84200d4b","deepnote_cell_type":"text-cell-h3"},"source":"### Cleaning Comments","block_group":"d923024aed8342dab851b031bc621cab"},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1703963689560,"execution_millis":6,"deepnote_to_be_reexecuted":false,"cell_id":"3c49273ab3fd433093d101e1904246fc","deepnote_cell_type":"code"},"source":"#Function to clean comment body\ndef clean_reddit_comment(text):\n    # Removing links that might interfere with tokenization\n    text = re.sub(r'http\\S+', '', text)\n    # Removing usernames to reduce noise\n    text = re.sub(r'@[^\\s]+', '', text)\n    # Removing new lines\n    text = text.replace('\\n', ' ')\n    #Removing \"&#x200B;\" character\n    text = text.replace(\"&#x200B;\", \"\")\n    # Removing symbols or characters that might not contribute to the analysis.\n    text = re.sub(r'[^a-zA-Z0-9\\s.]', '', text)\n\n    # Check if text is empty after cleaning\n    if not text.strip():\n        return None\n    \n    return text","block_group":"1e9b2d361e9d40c9af4bbffe2e00607e","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1703978889969,"execution_millis":14,"deepnote_to_be_reexecuted":false,"cell_id":"554dfc359fae449b874e365706b37260","deepnote_cell_type":"code"},"source":"all_comments['cleaned_comment'] = all_comments['comment_body'].apply(clean_reddit_comment)\n\n# Remove rows with empty or whitespace-only cleaned comments\ncleaned_comments = all_comments.dropna(subset=['cleaned_comment']).copy()\n\n# Display the cleaned DataFrame\nprint(all_comments[['comment_body', 'cleaned_comment']])","block_group":"04bb76ad3c74486caf859e76a3c464fd","execution_count":null,"outputs":[{"name":"stdout","text":"                                          comment_body  \\\n0    Perhaps this speaks to my naivety, but the pro...   \n1    > that it would resent it's exploitation and p...   \n2    â€œOnce men turned their thinking over to machin...   \n3    You're not wrong. We are in much more short-te...   \n4    Im not really a finance person, but in my opin...   \n..                                                 ...   \n281  What are you talking about. You can't be serio...   \n282  >I'm a data scentust and engineer.\\n\\nIf you'r...   \n283  No you're a person that believes artificial in...   \n284  >that believes artificial intelligence is aliv...   \n285  Do you know what a query is? In data science? ...   \n\n                                       cleaned_comment  \n0    Perhaps this speaks to my naivety but the pros...  \n1     that it would resent its exploitation and plo...  \n2    Once men turned their thinking over to machine...  \n3    Youre not wrong. We are in much more shortterm...  \n4    Im not really a finance person but in my opini...  \n..                                                 ...  \n281  What are you talking about. You cant be seriou...  \n282  Im a data scentust and engineer.  If youre a d...  \n283  No youre a person that believes artificial int...  \n284  that believes artificial intelligence is alive...  \n285  Do you know what a query is In data science It...  \n\n[286 rows x 2 columns]\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1703978897416,"execution_millis":1216,"deepnote_to_be_reexecuted":false,"cell_id":"098e36169a2341fcb4bbd00f3f4eeab2","deepnote_cell_type":"code"},"source":"# Create a new DataFrame for cleaned comments\ncleaned_comments = all_comments[['subreddit', 'post_title', 'post_sentiment', 'comment_id', 'parent_id', 'comment_author','comment_body','cleaned_comment', 'comment_score', 'comment_created_utc']]\n\n# Specify the path for saving the cleaned comments CSV file\ncleaned_comments_csv_path = '/work/GitHub_ML_Deepnote/Machine Learning/2. Cleaning & Pre-processing/cleaned_comments.csv'\n\n# Save the cleaned comments DataFrame to a new CSV file\ncleaned_comments.to_csv(cleaned_comments_csv_path, index=False, encoding='utf-8')\n","block_group":"e27236b941604eb291ab77ec73ab04c6","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1703979012273,"execution_millis":1106,"deepnote_to_be_reexecuted":false,"cell_id":"85aec356a0824075b8c0dd741fbf79d3","deepnote_cell_type":"code"},"source":"only_cleaned_comments = cleaned_comments[['cleaned_comment']].copy()\n\n# Display the new DataFrame\nprint(only_cleaned_comments)\n\n# Specify the path for saving the new cleaned comments CSV file\nonly_cleaned_comments_csv_path = '/work/GitHub_ML_Deepnote/Machine Learning/2. Cleaning & Pre-processing/only_cleaned_comments.csv'\n\n# Save the new cleaned comments DataFrame to a new CSV file\nonly_cleaned_comments.to_csv(only_cleaned_comments_csv_path, index=False, encoding='utf-8')","block_group":"717521ad7b2947f8aae3d7abbe871fd6","execution_count":null,"outputs":[{"name":"stdout","text":"                                       cleaned_comment\n0    Perhaps this speaks to my naivety but the pros...\n1     that it would resent its exploitation and plo...\n2    Once men turned their thinking over to machine...\n3    Youre not wrong. We are in much more shortterm...\n4    Im not really a finance person but in my opini...\n..                                                 ...\n281  What are you talking about. You cant be seriou...\n282  Im a data scentust and engineer.  If youre a d...\n283  No youre a person that believes artificial int...\n284  that believes artificial intelligence is alive...\n285  Do you know what a query is In data science It...\n\n[286 rows x 1 columns]\n","output_type":"stream"}]},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"e233ba1fd56c483da70c0db5849b62bc","deepnote_cell_type":"text-cell-h1"},"source":"# Pre-Processing: Tokenisation & Lemmatisation ","block_group":"409832967b784450b7e888107a00695a"},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1703968124700,"execution_millis":10,"deepnote_to_be_reexecuted":false,"cell_id":"a19148f2bc7746be95f937099939949e","deepnote_cell_type":"code"},"source":"# Creating a new dataframe to store tokenisation and lemmas\nword_level_df = cleaned_comments.copy()","block_group":"f0b24c3f94ba45d6bd02aa804ca8c8b7","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1703968126411,"execution_millis":31,"deepnote_to_be_reexecuted":false,"cell_id":"799a74877a184c53bec7f49de255f1ff","deepnote_cell_type":"code"},"source":"#Ensuring lemmatisation works correctly -- previously it assumed everything was a noun and didn't lemmatise words like \"thinking\" correctly.\n\n# Function to get the POS tag for each token\ndef get_pos_tag(token):\n    tag = pos_tag([token])[0][1][0].upper()\n    tag_dict = {\"J\": 'a', \"N\": 'n', \"V\": 'v', \"R\": 'r'}\n    return tag_dict.get(tag, 'n')  # Default to 'n' (noun) if the tag is not found\n\n# Modified lemmatization function with POS tagging\ndef process_text_with_pos_tags(text):\n    words = word_tokenize(text)\n    words = [word.lower() for word in words if word.lower() not in stops and word not in punct]\n    lemmatized_words = [lemmatizer.lemmatize(word, get_pos_tag(word)) for word in words]\n    return lemmatized_words\n","block_group":"a7bc629502f041f3ba7c61cc33d23dbe","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1703968128111,"execution_millis":5422,"deepnote_to_be_reexecuted":false,"cell_id":"9bc5543550734a5a87878bf05001fb3c","deepnote_cell_type":"code"},"source":"# Tokenisation and Lemmatization with a check for string-like objects\n\n# Tokenization \nword_level_df['tokens'] = word_level_df['cleaned_comment'].apply(lambda x: [word.lower() for word in word_tokenize(str(x)) if word.lower() not in stops and word not in punct] if isinstance(x, (str, bytes)) else [])\n\n# Lemmatization with POS tagging\nword_level_df['lemmas'] = word_level_df['tokens'].apply(lambda tokens: [lemmatizer.lemmatize(word, get_pos_tag(word)) for word in tokens])\n\n# Display the DataFrame\nprint(word_level_df[['cleaned_comment', 'tokens', 'lemmas']])","block_group":"a68c224fcd9b4a2594e9072ca51b2630","execution_count":null,"outputs":[{"name":"stdout","text":"                                       cleaned_comment  \\\n0    Perhaps this speaks to my naivety but the pros...   \n1     that it would resent its exploitation and plo...   \n2    Once men turned their thinking over to machine...   \n3    Youre not wrong. We are in much more shortterm...   \n4    Im not really a finance person but in my opini...   \n..                                                 ...   \n281  What are you talking about. You cant be seriou...   \n282  Im a data scentust and engineer.  If youre a d...   \n283  No youre a person that believes artificial int...   \n284  that believes artificial intelligence is alive...   \n285  Do you know what a query is In data science It...   \n\n                                                tokens  \\\n0    [perhaps, speaks, naivety, prospect, homicidal...   \n1    [would, resent, exploitation, plot, overthrow,...   \n2    [men, turned, thinking, machines, hope, would,...   \n3    [youre, wrong, much, shortterm, danger, bad, h...   \n4    [im, really, finance, person, opinion, major, ...   \n..                                                 ...   \n281  [talking, cant, serious, youre, saying, gpt, a...   \n282  [im, data, scentust, engineer, youre, data, sc...   \n283  [youre, person, believes, artificial, intellig...   \n284  [believes, artificial, intelligence, alive, co...   \n285  [know, query, data, science, doesnt, think, ge...   \n\n                                                lemmas  \n0    [perhaps, speaks, naivety, prospect, homicidal...  \n1    [would, resent, exploitation, plot, overthrow,...  \n2    [men, turn, think, machine, hope, would, set, ...  \n3    [youre, wrong, much, shortterm, danger, bad, h...  \n4    [im, really, finance, person, opinion, major, ...  \n..                                                 ...  \n281  [talk, cant, serious, youre, say, gpt, alive, ...  \n282  [im, data, scentust, engineer, youre, data, sc...  \n283  [youre, person, believe, artificial, intellige...  \n284  [believe, artificial, intelligence, alive, con...  \n285  [know, query, data, science, doesnt, think, ge...  \n\n[286 rows x 3 columns]\n","output_type":"stream"}]},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"4906119621d640d6b48e30d197a67d88","deepnote_cell_type":"text-cell-h3"},"source":"### Saving tokens and lemmas as a list of lists","block_group":"10850732ca7246088d995ad5bf85d87d"},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1703968635804,"execution_millis":11,"deepnote_to_be_reexecuted":false,"cell_id":"dd262c3c30af448cb20bd7ee752a8563","deepnote_cell_type":"code"},"source":"# Columns to clean\ncolumns_to_clean = ['cleaned_comment', 'tokens', 'lemmas']\n\n# Apply the cleaning operation to each column\nfor column in columns_to_clean:\n    word_level_df[column] = word_level_df[column].apply(lambda x: [item.encode('utf-8', 'ignore').decode('utf-8') if isinstance(item, str) else item for item in x] if isinstance(x, list) else x)","block_group":"e241da9d2e494811ad98f357e0f2e3b5","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1703969170345,"execution_millis":483,"deepnote_to_be_reexecuted":false,"cell_id":"e1ee3647262a4f6695922a61841e3eb8","deepnote_cell_type":"code"},"source":"#Re-arranging columns \nword_level_df = word_level_df[['subreddit', 'post_title', 'post_sentiment', 'comment_id', 'parent_id', 'comment_author', 'tokens', 'lemmas', 'comment_score', 'comment_created_utc']]\n\n# Specifying the path for saving the preprocessed data CSV file\npreprocessed_data_csv_path = '/work/GitHub_ML_Deepnote/Machine Learning/2. Cleaning & Pre-processing/word_level_df.csv'\n\n# Savinb the preprocessed data DataFrame to a new CSV file\nword_level_df.to_csv(preprocessed_data_csv_path, index=False, encoding='utf-8')","block_group":"06bddc2b80b04a8b96dbeb68ee9852e7","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"06eecbe2b6164c88921b5e47c420bc2d","deepnote_cell_type":"text-cell-h3"},"source":"### Saving tokens and lemmas as a string","block_group":"d4fd4bc08d0f4ad2a39e363006d564e4"},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1703969173180,"execution_millis":423,"deepnote_to_be_reexecuted":false,"cell_id":"0a91289fbc764dc5bd6b939c078462df","deepnote_cell_type":"code"},"source":"sentence_level_df = word_level_df.copy()\n\n# Join up the 'tokens' and 'lemmas' columns and replace the original columns\nsentence_level_df['tokens'] = sentence_level_df['tokens'].apply(lambda tokens: ' '.join(tokens))\nsentence_level_df['lemmas'] = sentence_level_df['lemmas'].apply(lambda lemmas: ' '.join(lemmas))\n\n# Specify the path for saving the CSV file\ncsv_file_path = '/work/GitHub_ML_Deepnote/Machine Learning/2. Cleaning & Pre-processing/sentence_level_df.csv'\n\n# Save the DataFrame to a CSV file\nsentence_level_df.to_csv(csv_file_path, index=False, encoding='utf-8')","block_group":"f81f0431f4ce44499b71306b0a1fb335","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=f64215d6-debc-46bd-b273-63565459a66d' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote":{},"orig_nbformat":2,"deepnote_notebook_id":"71174bacb7e342fba3733d35232a5b61","deepnote_persisted_session":{"createdAt":"2023-12-31T12:45:09.916Z"},"deepnote_execution_queue":[]}}