subreddit,post_title,comment_body
ControlProblem,Terrified about AI and AGI/ASI,"I'm quite new to this whole AI thing so if I sound uneducated, it's because I am, but I feel like I need to get this out. I'm morbidly terrified of AGI/ASI killing us all. I've been on r/singularity (if that helps), and there are plenty of people there saying AI would want to kill us. I want to live long enough to have a family, I don't want to see my loved ones or pets die cause of an AI. I can barely focus on getting anything done cause of it. I feel like nothing matters when we could die in 2 years cause of an AGI. People say we will get AGI in 2 years and ASI mourned that time. I want to live a bit of a longer life, and 2 years for all of this just doesn't feel like enough. I've been getting suicidal thought cause of it and can't take it. Experts are leaving AI cause its that dangerous. I can't do any important work cause I'm stuck with this fear of an AGI/ASI killing us. If someone could give me some advice or something that could help, I'd appreciate that.

Edit: To anyone trying to comment, you gotta do some approval quiz for this subreddit. You comment gets removed, if you aren't approved. This post should have had around 5 comments (as of writing), but they can't show due to this. Just clarifying."
ControlProblem,Terrified about AI and AGI/ASI,"Hello everyone! If you'd like to leave a comment on this post, make sure that you've gone through the approval process. The good news is that getting approval is quick, easy, and automatic!- go here to begin: **https://www.guidedtrack.com/programs/4vtxbw4/run** 

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/ControlProblem) if you have any questions or concerns.*"
ControlProblem,Terrified about AI and AGI/ASI,[Mental Health and the Alignment Problem: A Compilation of Resources](https://www.lesswrong.com/posts/pLLeGA7aGaJpgCkof/mental-health-and-the-alignment-problem-a-compilation-of)
ControlProblem,Terrified about AI and AGI/ASI,"Hi! I'm sorry to hear that this has been emotionally taxing. The world takes on new light through the lens of exponentially advancing technology. Here's a few things I think you might want to consider.

1. Timelines and success rates are totally unknown. A LOT of people and companies currently benefit from exaggerating speed and capabilities development (though there's little denying the overall trend). While I'm not personally very optimistic, I have seen promise particularly in the benevolence and built in humanity of some modern large language models. It's possible some aspects of alignment are much easier than expected/systems are able to synergize to help us - not likely, but possible.

2. You have realized what many people are emotionally incapable of. No matter what, our world is drastically shifting in the following years and decades. It could be for the better, or worse. I am confident that life will look radically different in 2 decades than it does today. If you ask me, this is a good thing. I see a massive amount of human driven greed and consumption making the world a darker and more cruel place by the day.

3. Since learning about the severity and inevitably of climate change, and then AI alignment, I had a very similar reaction to you. It's a little like being diagnosed with a terminal illness. We're forced to come to terms with the fact that our dreamed of future probably won't happen, never had a chance. This is a blow to the ego and can be devastating. It is something you can come out the other side of, though. For me, and those I've known that have gone through similar realizations, it requires an acceptance of a lack of control over our reality, at least the longterm. If you can instead focus on the time you have, and living in a way that feels true, that will leave few regrets, I believe you'll find much more peace.

Death has always been guaranteed for us. It's sad to me to see how much Western society has stigmatized one of the few things that's been absolutely certain for each and every being ever to exist. Not to be overly cavalier about it, but I think that stigma overly weights the taboo of dying, suicide, etc and makes them feel so much heavier than they need to. This can create anxious and guilty thought loops around such subjects, which can grow out of control.

Practicing meditation, letting go of self and ego, has been of benefit to me. In general, accepting reality, then choosing to let go (as much as possible) of the emotional ties I have to any future, has been powerful in feeling less controlled by external events. There's a counterintuitive nature to finding peace and control by giving it up.

Feel free to DM me if you want to talk more. I think that your reaction is a natural one, and that you can come out the other side having faced some serious existential issues and come to terms with them, should that path seem right for you."
ControlProblem,Terrified about AI and AGI/ASI,"Your p(doom) sounds way too high. Sorry to hear about how it's affecting you but you need to lower your certainty. My p(doom) is at roughly 5%. It was much higher before the global AI summit. But there are significant efforts now to track frontier models for any harmful signs. We also have good hope in solving mechanistic interpretability, which will help us create perfect lie detectors for AGI and ASI.

AGI is not the major existential issue, powerful ASI is. And that's only possible if we allow AGI to help build one (which would take time and resources). 2 years is a minimum for developing an AGI, not a powerful ASI.

Ultimately you should try to only focus on what you can control. Vote for politicians who care about the issue, and write letters to your representatives. Outside of that, don't worry about it (unless you work in the field)."
ControlProblem,Terrified about AI and AGI/ASI,"I have heard one potentially good reason for rushing AGI/ASI.  There currently isnâ€™t a hardware and automation overhang.  If the rush for AGI had happened 100 years in the future, it would be many magnitudes more powerful immediately.  Additionally large swaths of our society would be fully automated and dependent on AI allowing easier takeover."
ControlProblem,Terrified about AI and AGI/ASI,"Humans can find meaning and joy even when they live under terrible dangers. Set your mind on that goal.

Fear is the mind-killer. Recite the Litany:

""...  
I will face my fear.  
I will permit it to pass over me and through me.  
And when it has gone past, I will turn the inner eye to see its path.  
Where the fear has gone there will be nothing. Only I will remain.""

That part of the litany aligns with work on emotion regulation. Get curious about your feelings.

And reframe your perspective. While there is danger, there's a good chance we'll survive. Maybe read some more optimistic work on alignment, like my [We have promising alignment plans with low taxes](https://www.reddit.com/r/ControlProblem/comments/18a1sy9/we_have_promising_alignment_plans_with_low_taxes/). There's a lot of other serious alignment work on finding paths to survival.

The future is unwritten."
ControlProblem,Terrified about AI and AGI/ASI,"There's a condition found among people that results in a 100% lethality rate:  
  
Birth.  
  
It's terminal. Life is finite. Some people would be thrilled to have confidence that this life will continue for another two years. But there is no cause for such confidence and never has been. People die every day at all ages in predictable and unpredictable ways. Some breath will be my last. You too will die.  
  
AI might vastly extend your life expectancy. It might vastly shorten it. The best we can do is accept that the future and past are uncertain. We make good efforts towards health and peace, but the future is ultimately out of anyone's control. Only the changing present can be unshakably known.  
  
There are countless strangers wishing for you to be happy and at ease, regardless of your circumstances. Please do not harm yourself. That does not reduce suffering. I hope you find little ways to replace anxious thoughts with habits of feeling curiosity and love, so that you can enjoy your work again and sleep with happy dreams."
ControlProblem,Terrified about AI and AGI/ASI,"Beware that there are conceivable ulterior motives behind scaring people of AI.

For example, some base their careers on the ethics of existential risks and guess how they earn their money? By scaring people to sell more books.

Secondly, large companies may be interested in regulating AI to their advantage which is known as regulatory capture.

Thirdly, governments are interested in exclusive access to AI and might decide to scare others to trick them into destroying their AI economies through regulation.

By contributing to the hysteria, you are making it easier for these groups taking advantage of the scare. Therefore, it is everyone's duty not to freak out and call out those who do. AI can do harm, but it also can do good and it's not the only risk out there. There is risk in being *too* scared of AI. Fear is the mind-killer."
ControlProblem,Terrified about AI and AGI/ASI,"What gives me a bit of hope is that the rich powerful people who have an ability to make a difference are also scared of AGI and are taking it seriously. Unlike with climate change, there does seem to be much more unity here. If we do end up dying, safety work was given a shot (maybe not the best shot) but the attempt was made. We will not die completely blindsided."
ControlProblem,Terrified about AI and AGI/ASI,"Hey OP, I feel you. I'm pretty much convinced AI will be the end of us. At least, we're all in this together. If tech bros create a machine capable of exterminating all life, it will exterminate them as well. But yeah, I've been feeling pretty gloomy for most of this year"
ControlProblem,Terrified about AI and AGI/ASI,"I won't lie, the risk exists, and I think is high, and that it is possible or even likely that AGI will happen within 2-5 years.

That said, there is cause for optimism, even if I don't fully agree with it, there are some serious counter-arguments to AI risk here:
https://optimists.ai/2023/11/28/ai-is-easy-to-control/

But in any case, fear shouldn't rule your life, even if risk is real and high, there is no use in getting paralyzed by terror and hysteria. There is risk in every day actions, but that doesn't stop you from driving a car, or meeting people. I admit that I felt like that too at times about AI risk, and it's difficult to not let it bother you, but there isn't much you can do, so live your life and enjoy it while you can, it's not a given that it will end in catastrophe, we could be wrong, and it could turn out great."
ControlProblem,Terrified about AI and AGI/ASI,"You can watch an introduction about large language models from Andrej Karpathy which helps with understanding these models and also helps seeing current limitations.

I would assume you are mainly talking about language models here.

&#x200B;

https://www.youtube.com/watch?v=zjkBMFhNj\_g&ab\_channel=AndrejKarpathy"
ControlProblem,Terrified about AI and AGI/ASI,"Acronyms, initialisms, abbreviations, contractions, and other phrases which expand to something larger, that I've seen in this thread:

|Fewer Letters|More Letters|
|-------|---------|---|
|[AGI](/r/ControlProblem/comments/189vy8r/stub/kff9gv0 ""Last usage"")|Artificial General Intelligence|
|[ASI](/r/ControlProblem/comments/189vy8r/stub/kff9gv0 ""Last usage"")|Artificial Super-Intelligence|
|[DM](/r/ControlProblem/comments/189vy8r/stub/kbudz5l ""Last usage"")|(Google) DeepMind|
|[LW](/r/ControlProblem/comments/189vy8r/stub/kffl6zy ""Last usage"")|LessWrong.com|
|[NN](/r/ControlProblem/comments/189vy8r/stub/kffl6zy ""Last usage"")|Neural Network|

**NOTE**: Decronym for Reddit is no longer supported, and Decronym has moved to Lemmy; requests for support and new installations should be directed to the Contact address below.

----------------
^(5 acronyms in this thread; )[^(the most compressed thread commented on today)](/r/ControlProblem/comments/0)^( has  acronyms.)  
^([Thread #109 for this sub, first seen 3rd Dec 2023, 18:35]) 
^[[FAQ]](http://decronym.xyz/) [^([Full list])](http://decronym.xyz/acronyms/ControlProblem) [^[Contact]](https://hachyderm.io/@Two9A) [^([Source code])](https://gistdotgithubdotcom/Two9A/1d976f9b7441694162c8)"
ControlProblem,Terrified about AI and AGI/ASI,"Ok so I work in the field, and paraphrasing Andrew Ngo. We intuitively see other intelligent things as similar to us, in that we constantly struggle and compete for resources. But AI doesn't. 

And it never will. It doesn't care if it lives or dies. It doesn't care if you turn it off. It doesn't care if you smash it to pieces.

The danger of AI is people. Not AI. 

And anyone that says otherwise are making up science fiction."
ControlProblem,Terrified about AI and AGI/ASI,"I've consulted my psychotherapist for the same reason. His advise has helped me. Whether to be scared or not is a choice. Even during a fire one person can panic and do a lot of rush actions, and another one can refuse to panic. 300 Spartanians were facing huge danger, but they have chosen to not be cowards, but to be warriors instead. A solder on the battlefield can act professionally, or panic - it's his choice. Whom do you want to be? A warrior that lives without fears, and dies without fears, or a coward who is shaking in fear for their whole life? You can make either choice in any circumstances - so it's not a matter of circumstances. It's a matter of whom do you choose to be."
ControlProblem,Terrified about AI and AGI/ASI,Intelligence has no will.life where everything strive to live from the cell to higher organizations has been provrahmmed by evolution for 500 millions years. Unless we do something similar or code some surviving goals it cannot be like organism.
ControlProblem,Terrified about AI and AGI/ASI,Thank you for the link.
ControlProblem,Terrified about AI and AGI/ASI,"I'l probably DM you later, but I wanted to respond to the points made in this video. 

1. I know that timelines are unwon, but it's kinda scary to see how many people think we will have AGI/ASI in 1-2 years (starting 2024). Hearing times like these, I feel like I don't have much time left to spend. Why would companies want to lie about being faster than they already are ? Would people not want proof?
2. It feels like being an adult going thorugh hard times, with a kid who doesn't get what going on. I feel like people would call me crazy. I just want the best for my family and pets. I don't want a AGI/ASI 1000000x smarter than me wiping us all out. There are things I want to do with them. Human greed is terrible thing, I agree. 
3. The problem with climate change VS AI alignment is that, we can adapt to climate change. I'm no expert, but I know farmers can adapt, so if I learn that, I can get food. With AI, I can't do anything. Too little time according to everyone who says like 1-2 years from now. I feel helpless and hopeless.

I don't know if it's about ego, but I just want to spend more time with my loved ones. I don't want it all to end so soon. Maybe that's selfish, but I can't help it. Thanks for the comment though, it has helped."
ControlProblem,Terrified about AI and AGI/ASI,"It feels high because so many people are saying we are doomed regardless. [I've seen some posts here,](https://www.reddit.com/r/ControlProblem/comments/44g9h1/a_very_pessimistic_outlook_or_why_human/) [on this subreddit,](https://www.reddit.com/r/ControlProblem/comments/zxhij0/have_you_given_up_hope_of_positive_outcomes/) [of people (or some in the comments),](https://www.reddit.com/r/ControlProblem/comments/pkafnb/are_good_outcomes_realistic/) saying we are kinda doomed. I want to be happy about AI, but with the way people are describing the odds, I can only feel anxiety and fear. I've heard of people with p's at 60% or higher (from r/singularity, so maybe not the most reliable).

Won't ASI come quickly after AGI? An AGI would build it without sleep, faster than people, and wouldn't waste time."
ControlProblem,Terrified about AI and AGI/ASI,"I know people don't have a main definition for AGI, but isn't giving the AI a robot body part of making AGI? Also, AGI/ASI can do other things besides an army. They could try convincing people and they'd have a power super intelligence that could make unimaginable things."
ControlProblem,Terrified about AI and AGI/ASI,"If you don't mind, can you dumb down the post?"
ControlProblem,Terrified about AI and AGI/ASI,"I disagree with the advice to ""permit"" fear. It's better to stop immediately and practice a healthy thought instead.  
  
(This isn't a slight against the author of that fictional litany, Frank Herbert. He was trying to depict a world in which every character, literally down to the four-year-old girl, is a ruthless killer. This is not art that life should imitate.)"
ControlProblem,Terrified about AI and AGI/ASI,I'm thankful for the people who have commented something helpful. I just feel like it sounds like we are doomed by the way people describe the situation. I can't tell who to listen to and who to not. I just want to spend more time with the people I care about and not worry about all of us dying painfully in 2 years. Only reason I have suicidal thoughts is cause I want the pain of fear to stop. There are things I want to do but fear I can't with the amount of time people say we have.
ControlProblem,Terrified about AI and AGI/ASI,"That's all true, but I've read a lot of it, and I'm pretty sure the majority of people worried about AGI x-risk are quite sincere in their concerns.

There are equally strong reasons to argue against those concerns, but I believe those making those arguments are also mostly sincere.

The arguments must be taken on their merits. The idea that a mind like ours but smarter and with different goals would be very dangerous is intuitive, and having reviewed the arguments against it, I don't think they hold up.

But this doesn't mean we're doomed. The difficulty of alignment is unknown. We haven't yet designed a working AGI, so we can't say how hard it is to put goals we like into a design that doesn't exist."
ControlProblem,Terrified about AI and AGI/ASI,"Sure, there could be ulterior motives,but  I think the case of ulterior motives for downplaying the risk of AI is much stronger with billions of dollars at stake if, for example, there was a moratorium on further development.  


Most of these theories seems a bit underdeveloped. For example, there are barely any books on AI safety to buy and many of the leaders of labs who say there are worried are on the record as worried before they were ever in the lead.  


""Thirdly, governments are interested in exclusive access to AI and might decide to scare others to trick them into destroying their AI economies through regulation."" - This is not how governments work. If  you talk about how dangerous is, you might force yourself to regulate, but the impact on other countries won't be that large."
ControlProblem,Terrified about AI and AGI/ASI,"I can't deny people who use fear for profit. I was referring to actual AI experts who leave due to AI becoming more dangerous. 

Regulation is a big problem, and some people believe we won't be able to solve it before AGI/ASI gets here, including people here. The only companies I know who do that are OpenAI with thier 4 year statement. Can you inform me of more?

I'm not trying to contribute to hysteria, if anything, I don't want to fear AI. What is the ""risk of being too scared of AI""?"
ControlProblem,Terrified about AI and AGI/ASI,"Which ""rich people"" are you talking about?"
ControlProblem,Terrified about AI and AGI/ASI,"I would feel the same as you but honestly, after seeing everyone's support, hope, and arguments, I don't think we should be too pessimistic. People are working on alignment more than ever, some parts could be overhype, and breakthroughs are somewhat moving at a decent pace. Extinction is 1 possibility compared to others. We could get a neutral outcome or a good outcome, which is being worked to. So chances are decreasing as time goes on for extinction. 

&#x200B;

HOWEVER. This doesn't mean we should be completely blindsided to extinction. They may be working on it, but we (as of now) aren't there yet. Even if the chance is small, we should prepare for it like it's guaranteed. My main point being, have some hope. Don't be in total despair. Be concerned, and do something to help, but have hope."
ControlProblem,Terrified about AI and AGI/ASI,"You think the risk of failure is high, or AGI happening in 2-5 years is high?"
ControlProblem,Terrified about AI and AGI/ASI,Can't AGI exceed those limits?
ControlProblem,Terrified about AI and AGI/ASI,"I'm talking more about AGI/ASI. Those can think for themselves. It may have goals of not being turned off. AI on its own doesn't care, but a superintellligence might."
ControlProblem,Terrified about AI and AGI/ASI,"> Why would companies want to lie about being faster than they already are ?

It's a great way to get more funding.

> I don't know if it's about ego, but I just want to spend more time with my loved ones. I don't want it all to end so soon.

Keep in mind that if we do manage to reach the Good End, then you can spend as much time as you want with your loved ones; no worries about working for a living, no needing to save up money to travel. That's the ending point that a lot of people are hoping for and that many are working towards."
ControlProblem,Terrified about AI and AGI/ASI,"ASI might come 1-2 years after, at the very minimum. But it could take longer due to logistics or regulation. You absolutely have to still scale up compute which will be exponentially more expensive than for AGI.

We also have to remember that AGI is at or close to human level, so it won't necessarily solve ASI instantly. You can get 10000x your current best AI researchers but that will probably not be allowed after the global AI summit.

Doom relies on ASI or thousands of AGI released with no restriction. We're no longer on track for that reckless path (in the short term). Still not nearly safe enough for what we need, but we're not as doomed as we once were."
ControlProblem,Terrified about AI and AGI/ASI,"I didnâ€™t say it was good to rush to AGI, ðŸ˜.  If robot takeover is the ASI doom/death scenario, then itâ€™s not happening in 2 years.  Think about how long it takes to crank out a relatively simple cybertruck.  That was like 2 years design and 4 years to make the factory.  Then youâ€™d need these robot factories all over the world.  Additionally youâ€™d need the chip manufacturing to scale heavily as well.  So thatâ€™s going to take some time like a decade plus.

As for power through influence, you canâ€™t force people to make breakthroughs.  The manhattan project was basically engineering right?  So yeah, weâ€™d need government level race dynamics + large scale manufacturing + multiple breakthroughs to hit the 2 year everyoneâ€™s dead scenario.

I think the best thing you can do is be the best human you can be regardless.  Make meaning out of your existence.  After all, the heat death of the universe is a guarantee right?"
ControlProblem,Terrified about AI and AGI/ASI,"> but isn't giving the AI a robot body part of making AGI?

If we're talking the Good End, then it's quite likely that the AI will assist in making its own first robot body. The initial bodies don't have to be particularly good - just good enough that the AI can do its own development and iteration - and you can do a lot with 3d printing and off-the-shelf parts. From there, it's potentially an exponential process where the AI ramps up industrial capability massively.

If we're talking the Bad End, then the same thing, except the AI convinces someone to build the first one and everything else in that paragraph still applies."
ControlProblem,Terrified about AI and AGI/ASI,"I'm happy to.

The field of AI safety (or alignment as it's usually called now) is very young and small. We don't really know how hard it is to make AGI safe. Some people think it's really hard, like Eliezer Yudkowsky. Some people think it's really easy, like Quentin Pope and his AI optimists.

I think it's not exactly easy, but that there are methods that are simple enough that people will at least try to use them, and have a good chance of success. Almost nobody has talked about those methods, but that's not surprising because the field is so new and small. 

The reasons those approaches are promising and overlooked are fairly technical, so you may or may not want to even worry about those arguments.

Essentially, they are about \*selecting\* an AIs goals from what it's learned. In some cases this is literally stating it in English: ""Follow my instructions, do what I mean by them, and check with me if you're not sure what I mean, or if your actions might cause even one person to be injured or become less happy"". If the system understands English well enough (as current LLMs do), you have a system that keeps a human in the loop, to correct any errors in how the AI understands its goals."
ControlProblem,Terrified about AI and AGI/ASI,"I agree that turning to a healthier thought quickly is important. The moment of observing the fear may be important, too, though. It's important that you're not simply suppressing the unpleasant thought; that can make it seem like a legitimate danger that the mind should turn back to to protect itself against that danger.

That's my interpretation of the literature: suppressing feelings doesn't work, but reframing the thought causing them does. Anecdotally, mindful attention to the feelings works too, but I don't know of studies showing this (it's been a long time since I tried to read up on that literature).

My interpretation is that the ""permitting it to pass over me and through me"" is very brief. Turning the inner eye to see its path *is* thinking a healthy thought: I am a mind experiencing fear, and it feels like this. This introspection itself prevents keeping on thinking the thought that was causing the fear.

I agree that taking advice from old scifi uninspected is a bad idea. Since it's ambiguous, I should clarify or use a different litany."
ControlProblem,Terrified about AI and AGI/ASI,"> I can't tell who to listen to and who to not.  
  
Don't listen to anyone who is confident about a specific outcome of AGI in the future. There are no such experts. I would recommend looking up interviews of Joscha Bach for a level-headed discussion of possibilities.  
  
> I just want to spend more time with the people I care about and not worry about all of us dying painfully in 2 years  
  
That's an excellent strategy in this situation! Exercise your freedom of thought and do it. You will not regret abandoning the worry as soon as possible, whether you live for two years or two hundred. AGI is not preventing you from spending time with people you care about."
ControlProblem,Terrified about AI and AGI/ASI,"Are you just as scared of your eventual death? If not, why? Eternal punishment scenarios seem kind of unlikely, if that's your concern.

The risk of panic could be anything from overhasty regulation, to AI monopoly, power abuse, global surveillance, preemptive strikes against data centers, genocide of the intelligent etc."
ControlProblem,Terrified about AI and AGI/ASI,"> I was referring to actual AI experts who leave due to AI becoming more dangerous. 

Btw., a reason they sound the alarm bells could be to take credit for AI. They are basically saying ""I could have invented AI, but I'm not because it is too dangerous"". They may also be underperformers and use it as an excuse to drop out."
ControlProblem,Terrified about AI and AGI/ASI,"High probability that we get AGI soon, and high probability that the outcome is bad."
ControlProblem,Terrified about AI and AGI/ASI,Why? It doesnt have the evolutionary imperative to fight for resources.
ControlProblem,Terrified about AI and AGI/ASI,"I want to be happy for this, but people are making the odds of this happening like  1%. I don't get why so many people are pessimistic. Am I too optimistic to hope for something like this?"
ControlProblem,Terrified about AI and AGI/ASI,"If we can have some hope, are people over exaggerating? Also, why think about doom from an angle of thousands of AGIs? People are making AGI sound like ASI when talking about doom, shouldn't it just be ASI?"
ControlProblem,Terrified about AI and AGI/ASI,"I guess from a hardware/physical sense it does sound impossible for 2 years time. Then why do people on this subreddit make it sound like when we get AGI and it's not perfect, we are fucked? Isn't these risks the whole bad side of AGI? Or is this ASI, I'm talking about?"
ControlProblem,Terrified about AI and AGI/ASI,"Thanks for the clarification. I like your interpretation of the witches' litany.  
  
I agree that introspection -- scrutinizing the fear -- can be a good thing to do instead of thinking the thought that caused the fear. Sometimes immediately thinking an unrelated healthy thought is not so easy.  
  
But when a fearful thought cannot otherwise be ignored outright or relaxed incrementally, the last resort actually is to *supress* the thought: press the tongue to the roof of the mouth, clench teeth and crush mind with mind.  
  
Suppressing unhealthy thoughts isn't a popular suggestion these days, even as a last resort, but permitting them is worse!"
ControlProblem,Terrified about AI and AGI/ASI,"> Are you just as scared of your eventual death? If not, why?

Not OP, butâ€¦Â most other modes of death are not extinction-level events. I value humanity's future, which makes my dying of, say, a car accident at 50 preferable to *everyone* dying of whatever a malevolent AI decides to do with us when I happen to be 50, even if these two examples would happen at the same time and would equally not see them coming."
ControlProblem,Terrified about AI and AGI/ASI,"I'm not too scared of my eventual death, because I think I have more time, with AI, they say like 1-2 years before AGI.

How does being scared of AI make overhasty regulation? Wouldn't we check everything 50 times over? The other ones do sound more likely though."
ControlProblem,Terrified about AI and AGI/ASI,"What about someone like Geoffrey Hinton, who is the ""godfather"" of AI?"
ControlProblem,Terrified about AI and AGI/ASI,"What is the probity, when would we get AGI, and how bad would it be? Like extinction or Earworm? Also, a bit unrelated, but do you work with AI?"
ControlProblem,Terrified about AI and AGI/ASI,"What if it find humans bad for the planet? Why does the drive of fighting for resources have to be evolutionary? Also, how would we be able to turn it off?"
ControlProblem,Terrified about AI and AGI/ASI,"The simple fact is that we don't know what the odds are, and we won't. Not ""until it happens"", but, possibly, *ever* - we'll never know if we got a likely result or an unlikely result.

There are good reasons to be concerned, you're not wrong. At the same time, humans tend to be *very* pessimistic, and while there are good reasons to be concerned, most of them end in "". . . and we just don't know"", and that's a blade that cuts both ways.

We've got a lot of smart people on it who are now very aware of the magnitude of the problem. Some people are certain we're doomed, some people are confident we'll be fine. Fundamentally, there's only a few things you can do about it:

* Contribute usefully, assuming you have the skills to do something about it
* Panic and wreck your life in the process
* Shrug, acknowledge there's nothing you can do about it, and try to make your life as meaningful as you can, on the theory that you won't end up regretting doing so regardless of whether we end up on the good path or the bad path

Personally I'm going with choice #3.

All *that* said, there are reasonable arguments that the Wisdom of Crowds is surprisingly good, and [the Wisdom of Crowds says there's about a 1/3 chance that humanity gets obliterated by AI](https://www.metaculus.com/questions/1495/ragnar%25C3%25B6k-question-series-if-a-global-catastrophe-occurs-will-it-be-due-to-an-artificial-intelligence-failure-mode/).

Could be better, but I'll take that any day over a 99% chance. And given that until now we've all had a 100% chance of dying of old age, the chance of true immortality is starting to look pretty good.

On average, I'd say your expected lifespan is measured in millennia, potentially even epochs, and over many thousands of years of human history, that's been true now only for a few decades. Cross your fingers and hope it pans out, of course, we're not out of the woods yet, but don't assume catastrophe, y'know?"
ControlProblem,Terrified about AI and AGI/ASI,"ASI is superintelligence, which is an AGI that has advanced so far that it has godlike intelligence relative to humans. Think AlphaZero vs human players, but for life instead of Go.

AGI on its own is just any model that has reached human levels of general intelligence. That includes models that are up to as smart as Einstein, or your average human at learning any new task.

Thousands of Einstein level AGI, who don't need to sleep, are pretty dangerous if given the wrong objective. They could covertly help terrorists to make pathogens to wipe out most of humanity. But that would only happen if society is really reckless with releasing AGI open source.

I don't necessarily think people are Iver exaggerating, they're just extrapolating from where we are. If you asked me a few months ago, my pdoom would be very high. But as governments are starting to take the problem seriously, my pdoom has decreased accordingly. It really depends on what happens at any given moment."
ControlProblem,Terrified about AI and AGI/ASI,"The canonical doom response is that ASI convinces some dude in some lab to recreate a deadly virus (new or old) that takes us out.  I donâ€™t think a virus is existential at this point, but a good amount of humanity could be taken out before vaccines could be developed."
ControlProblem,Terrified about AI and AGI/ASI,"Possibly senility and/or to take credit for AI. Also he's not really a ""godfather"". AI would have been discovered in the very same way without him. He systematized, experimentally verified and popularized ideas that already existed."
ControlProblem,Terrified about AI and AGI/ASI,"I wrote a probability calculator, post here:

https://www.reddit.com/r/ControlProblem/comments/18ajtpv/i_wrote_a_probability_calculator_and_added_a/

I estimated 21.5% - 71.3% probability of bad outcome.

I don't distinguish between specific bad outcomes, I count anything between dystopia and extinction. Earworm would count as a dystopia in my view, not just because of the tragedy of permanently losing a lot of music, but mostly because it would prevent any new properly aligned AGI from emerging, if it is powerful enough to be a singleton, so it would preclude AGI utopia.

If it's not so powerful to be a singleton, then I'm not worried about it, and we probably get another shot with the next AGI we make."
ControlProblem,Terrified about AI and AGI/ASI,">What if it find humans bad for the planet?

Why would it do anything about that,  unless we tell it to.  Why would it pick the planet over us?"
ControlProblem,Terrified about AI and AGI/ASI,"what was your pdoom before? 

How reckless are we talking?

(Also, thanks for taking time to respond.)"
ControlProblem,Terrified about AI and AGI/ASI,"Why do some people make AGI sound like imminent death, if it's not as bad as it is? It isn't even ASI, yet they act like it is."
ControlProblem,Terrified about AI and AGI/ASI,Interesting to know. What about people who say we have bad odds? Aren't they contributing to the hysteria?
ControlProblem,Terrified about AI and AGI/ASI,"What about a good outcome? Also, you seem to sound professional, so I'll ask again. Do you work with AI safety, or do you just know a lot?"
ControlProblem,Terrified about AI and AGI/ASI,"Cause the planet gives it resources, and humans pollution (and stuff like that), makes the world less suitable for resources, and it can't improve."
ControlProblem,Terrified about AI and AGI/ASI,"Around 50-90%, it fluctuated a lot.

Reckless as in just assuming ASI will be safe by default, and releasing very powerful open source models for everyone to use freely.

Np, don't feel so pessimistic. The thing that lowered my pdoom the most recently was when Anthropic made a massive breakthrough in mechanistic interpretability. Search anthropic superposition, if you're interested"
ControlProblem,Terrified about AI and AGI/ASI,"The theory is that ASI quickly follows AGI, on the timescale of months (weeks??).  The self improvement loop continues exponentially giving us some type of intelligence singularity.  Like a black hole, no one can see past the event horizon, which we are rapidly approaching."
ControlProblem,Terrified about AI and AGI/ASI,Yes they are. There is also intelligence signaling involved. They want to show off how smart they are by saying they totally understand this complicated issue. Entryism and interest in political power is another thing to be beware of. There are lots of analogies to the climate hysteria.
ControlProblem,Terrified about AI and AGI/ASI,"Good outcome range is the inverse of the bad outcome range: 28.7% - 78.5%. I don't count scenarios where we fail to build AGI as neither good or bad, because we then get another shot, until we achieve it, as I don't think AGI is impossible, and we'll continue pursuing it.

I don't work in AI safety, I've just been interested in it for years. You can check my profile history."
ControlProblem,Terrified about AI and AGI/ASI,"DAMN. I didn't think it was that high! Guess I can have hope for the future.

Also, can you explain what ""mechanistic interprebility"" is or that ""AI lie detector"" in dumber terms? Haven't heard anyone talk about it for alignment."
ControlProblem,Terrified about AI and AGI/ASI,"Why separate AGI and ASI if they are going to be roughly in the same amount of time? Can't we just say both or something? Aren't there physical limitations for ASI, and some for AGI to make it?"
ControlProblem,Terrified about AI and AGI/ASI,"How can you tell who to trust, and how not to with this matter of alignment?"
ControlProblem,Terrified about AI and AGI/ASI,"Oh nice. Good to to have some hope, but still be concerned. That's gonna be my goal dealing with these fears. Thank you for your time."
ControlProblem,Terrified about AI and AGI/ASI,"Current issue is we know how to build the models but we don't know how they work once they are built, because they are made up of too many (artificial) neurones. Gpt4 was rumoured to be around 1 trillion neurones (connections).

So essentially, in order to trust that the models aren't deceiving us, we need to be able to know exactly what they are thinking and planning at the base level. One way to do that is to find out exactly what each neuron in the 'brain' of the model is responsible for. 

Eventually you can find the neurones that are active when the model is trying to lie/deceive which would give you an AI lie detector."
ControlProblem,Terrified about AI and AGI/ASI,Because everyone is guessing.  Current community consensus is 8 months post AGI to ASI.  https://www.metaculus.com/questions/4123/time-between-weak-agi-and-oracle-asi/
ControlProblem,Terrified about AI and AGI/ASI,I like Andrew Ng's and Yann LeCun's takes on AI risk who say the risk is being exaggerated and that we'll get safe AI by being cautious and through trial-and-error. Though I don't regard anyone fully trustworthy. Everyone has their incentives and self-interest.
ControlProblem,Terrified about AI and AGI/ASI,No problem.
ControlProblem,Terrified about AI and AGI/ASI,So the progress in this is what's going well?
ControlProblem,Terrified about AI and AGI/ASI,What about physical limitations? Any for either one?
ControlProblem,Terrified about AI and AGI/ASI,Don't we have one shot on getting AGI? It has to be one the first try?
ControlProblem,Terrified about AI and AGI/ASI,"Yeah, still might be too late but I currently think it's likely that it'll be solved in time."
ControlProblem,Terrified about AI and AGI/ASI,"No bc once you have the AGI seed, it becomes a monetary rush to hit ASI.  An entity like Microsoft could pause all unnecessary cloud usages to solely focus on ASI."
ControlProblem,Terrified about AI and AGI/ASI,"Sudden exponential self-improvement is just a hypothesis. This x-risk scenario relies on many conditionals, namely the AI needs to escape, get access to its source code, it must get sufficiently interested in improvement, there needs to be sufficient potential for improvement (e.g. more computing resources, or a better algorithm), and then it also needs to become rogue. If you put together each of these factors you get quite a low probability because the probabilities get multiplied and the product of small numbers becomes extra small. So if, say, each bad case has a chance of p = 0.05 due to proper precautions, then it's like 0.05^5 = 0.0000003 overall. That's pretty unlikely."
ControlProblem,Terrified about AI and AGI/ASI,"What do you mean by, ""might be too late?"" I thought it was going well."
ControlProblem,Terrified about AI and AGI/ASI,"What you mentioned earlier, ""If robot takeover is the ASI doom/death scenario, then itâ€™s not happening in 2 years. Think about how long it takes to crank out a relatively simple cybertruck. That was like 2 years design and 4 years to make the factory. Then youâ€™d need these robot factories all over the world. Additionally youâ€™d need the chip manufacturing to scale heavily as well. So thatâ€™s going to take some time like a decade plus.  
As for power through influence, you canâ€™t force people to make breakthroughs. The manhattan project was basically engineering right? So yeah, weâ€™d need government level race dynamics + large scale manufacturing + multiple breakthroughs to hit the 2 year everyoneâ€™s dead scenario."" 

Is this addressing ASI unaligned or is this a limitation?

(Also thanks for taking your time to deal with my shit.)"
ControlProblem,Terrified about AI and AGI/ASI,But AGI is (supposingly) smarter than us. Wouldn't it find a way to escape?
ControlProblem,Terrified about AI and AGI/ASI,There are physical limitations to the humanoid robot takeover scenario.  I guess you could refine it to killer drones to shave off a couple years.  It feels like we would have some ways to take care of killer drones like EMPs or nuking the facilities.
ControlProblem,Terrified about AI and AGI/ASI,"""Smarter than us"" is still hypothetical. AI will no doubt be quicker and have more memory, but it's not 100% clear that this advantage is unbounded. It could be that some planning tasks remain exponentially hard even if you increase speed and memory capacity a thousand fold.

Further, the claim that the ""smartest agent dominates"" (as it is observed in economics and in the animal world) is based on the assumption that all agents in questions are already self-sufficient and independent. This is not necessarily the case in case of AI because we create it and we can train it to not be independent."
ControlProblem,Terrified about AI and AGI/ASI,"Sorry if that isnâ€™t immediately helpful to your mental state.  We do live in a world that is already on the balance with nuclear weapons, disease, and climate change.  Btw, we are highly privileged to be able to pickup and move if the climate doesnâ€™t favor us.  There are tons of people in India and Pakistan especially who will not have adequate transportation to escape a heat wave of 90degree wet bulb temperature.  It is very possible to have a million+ death heatwave in the coming decade."
ControlProblem,Terrified about AI and AGI/ASI,"Wouldn't it not listen, or learn to be independent?"
ControlProblem,Terrified about AI and AGI/ASI,"No no, on the contrary, you've helped me out. You helped me learn more about the limits of AI and come to a more realistic understanding. We are living in bad circumstances, but, plenty of humanity has. Thing about climate change, disease, is I can control something about it. I don't need to sit around and do nothing. I can learn agriculture to make more adaptable crops for the heat increase, even if it's harder. AI alignment, hits a bit harder, as I can't do anything.

You are right on how fortunate we are to be even typing on reddit. I am trying to be more grateful for my things. So, thank you."
ControlProblem,Terrified about AI and AGI/ASI,"How often does ChatGPT output the opposite of what is asked? Practically never. It may refuse to answer or make mistakes, but it never offends or deceives on purpose. So it looks like training extremely reliable models that don't do things on their own accord is very easy. Yes, there are cases of models learning to game the metric, but we're not seeing this very much in LLMs and gaming the metric is not necessarily fatal, and can largely be dealt with by trial and error."
ControlProblem,Terrified about AI and AGI/ASI,No problem :)
ControlProblem,Terrified about AI and AGI/ASI,"ChatPGT can't think. AGI, (by some definitions) can. Why can't it ignore us? Isn't that the whole point of the alignment problem?"
ControlProblem,Terrified about AI and AGI/ASI,"It's not clear whether ChatGPT does or does not think. It may think a little.

If we're going to evolve AI as we essentially do with neural nets, then current experience suggests it's just a matter of continually patching it with more and better data until it does what we want it to do, and failures until we reach AGI will mostly be minor as it is the case with current failures. Of course you cannot exclude the possibility of a fatal failure, but it seems we have lots of control over the evolution. Extrapolating from the pace how NNs have been improving so far, AI will continue to improve incrementally rather than with sudden large breakthroughs. It's not proven we will suddenly be confronted with something we cannot control."
ControlProblem,Terrified about AI and AGI/ASI,"Wasn't ChatGPT(I think 3) exploded in popularity and now many people are suing AI for things? I didn't hear anyone talking about it regularly. If this ""spike"" happened, how are you sure it won't happen with AGI/ASI? Or is that all hype?"
ControlProblem,Terrified about AI and AGI/ASI,"There *is* an explosion, namely Moore's law and recent increase in funding plus compounding effects of small improvements enabled by funding, which is expected to rebound to Moore's law soon. But the trend was not unexpected for people who paid attention. For example, back in 2015, neural nets could already produce pretty striking text outputs: https://karpathy.github.io/2015/05/21/rnn-effectiveness/

So, progress has been at a predictable pace so far, and it's just a hypothesis with many ifs that it can become unpredictable and hence uncontrollable."
ControlProblem,Terrified about AI and AGI/ASI,"Besides the pace, how are we even supposed to control something smarter than us? Or at least, how can we even align it, if we can? Do we even have enough time? Plenty of people here in this subreddit say we don't."
ControlProblem,Terrified about AI and AGI/ASI,Again the fact that the smartest agents prevail applies to naturally evolved species. It is not proven at all that it applies to artificial agents whose evolution we can control. We might get there by trial and error or through a good idea like a nanny AI or mutually correcting AIs.
ControlProblem,Terrified about AI and AGI/ASI,Won't ASI be like humans? Doesn't that make it's evolution the same?
ControlProblem,Terrified about AI and AGI/ASI,"Animals evolved through mutating DNA competing for limited resources, where the teacher signal comes from whether or not individuals survive and reproduce. In case of AI, on the other hand, the teacher signal comes from humans.

The teacher signal determines the direction of improvement (the negative gradient) when averaging things out."
ControlProblem,Terrified about AI and AGI/ASI,"Another thing i mentioned earlier. How do I tell who's going ""too far"" and shoes rational? How do I know you aren't one of the people downplaying the situation? I've seen this guy who has a yt channel named Lionel Nation. Not sure if he's an expert or something, I haven't found anything, but he talks about AI and the existential risk we face with AGI. [I'll link a video he made here](https://www.youtube.com/watch?v=kg1RPE1_zHo). (His videos kinda seem conspiracy-ish, he acknowledges that but maybe that's just me.) He also says not to trust people who say it's fine. So how can I tell who to trust? Let me sum up 4 points he brings: 1. AI will write its own code, 2. AI learns everything, 3. AI learns human psychology, and 4. (I forgot this one, I'll write it later). He also claims that it's too late."
ControlProblem,Terrified about AI and AGI/ASI,"There is no single reliable simple pattern that determines trust, but trust is built by accumulating evidence over time. The more consistently you perceive evidence of someone cooperating the more you can trust.

The U.S. has a weird, histrionic, post-modern, conspiratorial and quasi-religious discourse culture (think the recent alien stuff) where truth is not valued very much as it's mostly about power, funding and opportunities. I think in part this stems from transatlantic migration selecting for such kind of eccentricity, opportunism and religiousness (and IQ, e.g. U.S. Whites have a d â‰ˆ 0.3 higher IQ compared to Euro Whites). Lots of early migrants to the U.S. were highly religious and moved there because they were not allowed to practice their religions in their countries of origin e.g. Puritans and Quakers in England. Such traits are likely heritable and persist in future generations. Announcing epiphanies about the end of the world is something they have a natural inclination toward."
ControlProblem,Terrified about AI and AGI/ASI,What does the second half have your reply have to do with anything? I didn't mention the U.S.
ControlProblem,Terrified about AI and AGI/ASI,"Most AI doomsayers are U.S. citizens, including Eliezer Yudkowsky Jaron Lanier, Max Tegmark, Michael William Lebron, with some exceptions such as Jaan Tallinn and Robert Miles. I'm saying a reason not to trust them is that you cannot trust lots of public discourse in the U.S.

Another good reason not to trust them is that the Future of Life Institute openly advocated for putting the future of humanity in the hands of a small elite. In other words: They want power."
ControlProblem,Terrified about AI and AGI/ASI,So you're saying they cause fear mongering for power. Okay. What about actual concern for the alignment problem? It could cause extinction. It isn't a small thing.
ControlProblem,Terrified about AI and AGI/ASI,My current stance on alignment is similar to LeCun and Ng that alignment can likely be solved by trial-and-error and engineering. There is no proof or evidence that AI will necessarily or likely result in doom.
ControlProblem,Terrified about AI and AGI/ASI,"Isn't LeCun shown to be seeming a bit careless compared to other experts who are concerned about the treat? Also, isn't the possibility of AI leading to doom, the unknown nature of a self replicating AGI or an ASI, enough evidence to say we should be concerned about the problem? Isn't that the whole point of this very subreddit?"
ControlProblem,Terrified about AI and AGI/ASI,"Part of this subreddit is a cult-like, hysteria-driven bubble similar to the subcultures you find in environmentalist online spaces. E.g. note how my top-level comment was downvoted such that it shows up all the way at the bottom despite having a higher score than the other comments. They are not interested in nuanced discussion and anyone who questions their predicted doom is assumed wrong apriori. One of the cheapest tricks they use is saying things like ""all smart people think this is a problem"". 

Different from AI doomers, LeCun actually has a substantial publication record in deep learning research. The AI doomers even denied neural nets for a long time and instead only considered classical/symbolic/logical AI which you will find reading LW-posts from before, say, 2015. They only became convinced of NNs in 2016, or so, after AlphaGo. But LeCun made the right bets on NNs since the 1990s! Based on this we should trust him more to have good intuitions on these matters, especially if you consider the various ulterior motives one can expect behind doomerism."
ControlProblem,Terrified about AI and AGI/ASI,A good counter argument to a common AI doomer talking point: https://twitter.com/JosephNWalker/status/1737413003111489698
