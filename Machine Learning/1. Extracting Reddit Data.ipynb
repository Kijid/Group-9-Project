{
  "cells": [
    {
      "cell_type": "code",
      "source": "import praw\nimport pandas as pd\n\nimport glob\nfrom textblob import TextBlob\nimport csv",
      "metadata": {
        "source_hash": null,
        "execution_start": 1704200942343,
        "execution_millis": 27,
        "deepnote_to_be_reexecuted": false,
        "cell_id": "0e8a569ee0434e3da3bb35ce7f77b4db",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null,
      "block_group": "a5ce9313c9274d5dbfa3db08f1c158f0"
    },
    {
      "cell_type": "markdown",
      "source": "This code fetches data from various Reddit posts and stores them into pandas DataFrame. The primary steps involved are:\n\n- Establish a connection to the Reddit API using the PRAW library and the provided credentials.\n- Define a function called `fetch_subreddit_data()`. This function accepts a subreddit name and a list of post URLs. For each post, it fetches the title, author, content, score, and creation time, and appends them to a DataFrame. The function also fetches the similar data for each comment in a post if it is not deleted. It uses the `praw.models.Comment` to ensure the replies are indeed comments.\n- Defines a list of subreddit names and a dictionary of post URLs for each subreddit.\n- Iterates over the subreddit names, fetches data using the `fetch_subreddit_data()` function, and saves the output DataFrame to a CSV file named after the subreddit. The `index=False` argument in `to_csv` function ensures that the indexes are not included in the output CSV file.",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "38de40b83bb74042af2727aed3366f73",
        "deepnote_cell_type": "text-cell-p"
      },
      "block_group": "71ceeb4ec02a496f9b2cb5014cd84886"
    },
    {
      "cell_type": "code",
      "source": "# Setting up PRAW with our Reddit app credentials\nreddit = praw.Reddit(\n    client_id='1Q7zYJ3FBAowuOvu_axlLA',        \n    client_secret='OZXwSMsECF-1ZhIvVLE98gjrVOPdXw', \n    user_agent='MoT-Group9',      \n)",
      "metadata": {
        "source_hash": null,
        "execution_start": 1704200944584,
        "execution_millis": 85,
        "deepnote_to_be_reexecuted": false,
        "cell_id": "02c848cf9e1e4084bd99b1ad3e632641",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null,
      "block_group": "e5fa40fe4be641a6ae54090dc65ab6b3"
    },
    {
      "cell_type": "markdown",
      "source": "### Functions necessary to retrieve data from pre-selected sub-reddit posts",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "544809e06b284606886e21f993ee8c68",
        "deepnote_cell_type": "text-cell-h3"
      },
      "block_group": "39be464a661f44258949ef702d481415"
    },
    {
      "cell_type": "markdown",
      "source": "### They have been edited since the first commit to extract less in order to be able to carry more links. ",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "50ccc659bbed4ef7a863672bddb133ad",
        "deepnote_cell_type": "text-cell-h3"
      },
      "block_group": "1af9137fcd8d42738b95e16b59468b91"
    },
    {
      "cell_type": "code",
      "source": "#this funciton will determine the sentiment of selected posts within the selected subreddits\ndef get_sentiment(text):\n    analysis = TextBlob(text)\n    # Classify the sentiment as positive, negative, or neutral\n    if analysis.sentiment.polarity > 0:\n        return 'positive'\n    elif analysis.sentiment.polarity < 0:\n        return 'negative'\n    else:\n        return 'neutral'",
      "metadata": {
        "source_hash": null,
        "execution_start": 1704202643856,
        "execution_millis": 6,
        "deepnote_to_be_reexecuted": false,
        "cell_id": "37cc34ff428a40ba9c7bdf30809e5c23",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null,
      "block_group": "00d6ab42a4264d81b57e1c2b005e0074"
    },
    {
      "cell_type": "code",
      "source": "def fetch_comments(submission, subreddit_name):\n    submission.comments.replace_more(limit=None)\n    comments_data = pd.DataFrame(columns=['subreddit', 'post_title','comment_body'])\n\n    for comment in submission.comments.list():\n        if isinstance(comment, praw.models.Comment) and comment.body != '[deleted]':\n            # Appending information about each comment to the DataFrame\n            comments_data = comments_data.append({\n                'subreddit': subreddit_name,\n                'post_title': submission.title,\n                #'post_sentiment': get_sentiment(submission.title),\n                #'comment_id': comment.id,\n                #'parent_id': comment.parent_id,\n                #'comment_author': comment.author.name if comment.author else '[deleted]',\n                'comment_body': comment.body,\n                #'comment_score': comment.score,\n                #'comment_created_utc': pd.to_datetime(comment.created_utc, unit='s')\n            }, ignore_index=True)\n\n    return comments_data",
      "metadata": {
        "source_hash": null,
        "execution_start": 1704202235484,
        "execution_millis": 71,
        "deepnote_to_be_reexecuted": false,
        "cell_id": "2f4084c512c647eeb8da3e1a83daaf2a",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null,
      "block_group": "a9a499ea79d044e6b09358af1464d4e2"
    },
    {
      "cell_type": "code",
      "source": "# Creating a function to fetch subreddit data - takes in subreddit name and list of post URLs provided below.\ndef fetch_subreddit_data(subreddit_name, post_urls):\n    columns = ['subreddit', 'post_title', 'comment_body']\n    comments_data = pd.DataFrame(columns=columns)\n\n    # Looping through the provided post URLs\n    for post_url in post_urls:\n        submission = reddit.submission(url=post_url)\n\n        # Appending information about the post to the DataFrame\n        comments_data = comments_data.append({\n            'subreddit': subreddit_name,\n            'post_title': submission.title,\n            #'post_sentiment': get_sentiment(submission.title),\n            #'comment_id': submission.id,\n            #'parent_id': None,\n            #'comment_author': submission.author.name if submission.author else '[deleted]',\n            'comment_body': submission.selftext,\n            #'comment_score': submission.score,\n            #'comment_created_utc': pd.to_datetime(submission.created_utc, unit='s')\n        }, ignore_index=True)\n\n        # Process comments and their replies\n        comments_data = comments_data.append(fetch_comments(submission, subreddit_name), ignore_index=True)\n\n    return comments_data",
      "metadata": {
        "source_hash": null,
        "execution_start": 1704202237202,
        "execution_millis": 6,
        "deepnote_to_be_reexecuted": false,
        "cell_id": "deef63f3472b4a22b27b12981d4a25ae",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null,
      "block_group": "b8494bd23222423b874e9a638b1f558a"
    },
    {
      "cell_type": "markdown",
      "source": "### Applying the functions to extract data",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "daff5d1e4a13422b88eb7b3e4d8a3951",
        "deepnote_cell_type": "text-cell-h3"
      },
      "block_group": "86d06fccc0e94a598a2b97ace446ec3d"
    },
    {
      "cell_type": "code",
      "source": "# Example usage:\nsubreddit_names = ['singularity', 'Futurology', 'ArtificialInteligence']\npost_urls_for_each_subreddit = {\n    'singularity': ['https://www.reddit.com/r/singularity/comments/132kgur/i_dont_fear_malicious_or_rogue_ai_i_fear_how_ai/', 'https://www.reddit.com/r/singularity/comments/12983il/the_reason_i_dont_fear_artificial_intelligence/'],\n    'Futurology': ['https://www.reddit.com/r/Futurology/comments/9z9g0j/why_do_we_fear_artificial_intelligence_andor/'],\n    'ArtificialInteligence': ['https://www.reddit.com/r/ArtificialInteligence/comments/17ddumf/what_is_expected_to_gain_from_an_ai_safety_summit/']\n}\n\ngithub_linked_folder = '/work/GitHub_ML_Deepnote/Machine Learning/1. Extracted Reddit Data/'",
      "metadata": {
        "source_hash": null,
        "execution_start": 1704202239912,
        "execution_millis": 28,
        "deepnote_to_be_reexecuted": false,
        "cell_id": "a83feb1b89fa4f6eab4bc8fb673c758d",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null,
      "block_group": "76ab537ca9964f12bb221f7d617cc8da"
    },
    {
      "cell_type": "code",
      "source": "# Creating cvs files categorising data by subreddit\nfor subreddit_name in subreddit_names:\n    comments_data = fetch_subreddit_data(subreddit_name, post_urls_for_each_subreddit[subreddit_name])\n\n    # Specify the path for saving CSV files within the GitHub-linked folder\n    csv_file_path = f'{github_linked_folder}{subreddit_name}_comments_data.csv'\n\n    # Save the CSV file\n    comments_data.to_csv(csv_file_path, index=False, encoding='utf-8')\n\n",
      "metadata": {
        "source_hash": null,
        "execution_start": 1704202243073,
        "execution_millis": 4234,
        "deepnote_to_be_reexecuted": false,
        "cell_id": "7264ad9169ee4e7b9c4e52f3b78820e5",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null,
      "block_group": "454b431ec0d14e8f9956fca6d99d1f98"
    },
    {
      "cell_type": "code",
      "source": "#Creating a csv file with containing data for all subreddits\nall_dataframes = []\n\nfor subreddit_name in subreddit_names:\n    comments_data = fetch_subreddit_data(subreddit_name, post_urls_for_each_subreddit[subreddit_name])\n    all_dataframes.append(comments_data)\n\n    # Specify the path for saving CSV files within the GitHub-linked folder\n    csv_file_path = f'{github_linked_folder}{subreddit_name}_comments_data.csv'\n\n    # Save the CSV file\n    comments_data.to_csv(csv_file_path, index=False, encoding='utf-8')\n\n# Concatenate all DataFrames into one\nall_comments_data = pd.concat(all_dataframes, ignore_index=True)\n\n# Specify the path for saving the combined CSV file\ncombined_csv_file_path = f'{github_linked_folder}0_all_comments_data.csv'\n\n# Save the combined CSV file\nall_comments_data.to_csv(combined_csv_file_path, index=False, encoding='utf-8')",
      "metadata": {
        "source_hash": null,
        "execution_start": 1704202727572,
        "execution_millis": 4001,
        "deepnote_to_be_reexecuted": false,
        "cell_id": "10ab1f1b8de041bb95fb7b4380a43ec0",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null,
      "block_group": "75b5fea026dc44838d8d858ec39eb9ab"
    },
    {
      "cell_type": "markdown",
      "source": "Will likely want to get rid of comment_authors and use comments ids instead",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "4a70b05beb22438ebb1f1addd2ea2a98",
        "deepnote_cell_type": "text-cell-p"
      },
      "block_group": "b599429acd7046a39b5f589542170a44"
    },
    {
      "cell_type": "markdown",
      "source": "### Creating a summary of subreddit posts",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "15aff740b7644fd6ae236535349f0ba2",
        "deepnote_cell_type": "text-cell-h3"
      },
      "block_group": "f33c1b908d2b44a2895ecf570ed597eb"
    },
    {
      "cell_type": "code",
      "source": "# Create a summary DataFrame with the total number of comments for each post\nsummary_data = all_comments_data.groupby(['subreddit', 'post_title']).size().reset_index(name='total_comments')\n\n# Add the 'url' column based on the predefined URLs\nsummary_data['url'] = summary_data['subreddit'].map(post_urls_for_each_subreddit)\n\n# Add a 'post_id' column using the index as a unique identifier\nsummary_data['post_id'] = summary_data.index.map(lambda x: f'{x+1:03d}')\n\n# Apply the get_sentiment function to each row and append the sentiment information\nsummary_data['post_sentiment'] = summary_data['post_title'].apply(get_sentiment)\n\n# Rearrange the order of columns\nsummary_data = summary_data[['subreddit', 'post_id', 'post_sentiment', 'post_title', 'total_comments', 'url']]\n\n# Specifying the path for saving the overall summary CSV file\noverall_summary_csv_file_path = '/work/GitHub_ML_Deepnote/Machine Learning/1. Extracted Reddit Data/0_subreddits_summary.csv'\n\n# Save the overall summary CSV file\nsummary_data.to_csv(overall_summary_csv_file_path, index=False)\n\n# Display the overall summary data\nprint(summary_data)",
      "metadata": {
        "source_hash": null,
        "execution_start": 1704202747985,
        "execution_millis": 119,
        "deepnote_to_be_reexecuted": false,
        "cell_id": "c90b5c1ee38840da8684d949c0cc2d9d",
        "deepnote_cell_type": "code"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "               subreddit post_id post_sentiment  \\\n0  ArtificialInteligence     001       positive   \n1             Futurology     002       negative   \n2            singularity     003       negative   \n3            singularity     004       negative   \n\n                                          post_title  total_comments  \\\n0  What is expected to gain from an AI safety sum...              59   \n1  Why do we fear artificial intelligence and/or ...              42   \n2  I don't fear malicious or rogue AI - I fear ho...             153   \n3    The reason I donâ€™t fear artificial intelligence              32   \n\n                                                 url  \n0  [https://www.reddit.com/r/ArtificialInteligenc...  \n1  [https://www.reddit.com/r/Futurology/comments/...  \n2  [https://www.reddit.com/r/singularity/comments...  \n3  [https://www.reddit.com/r/singularity/comments...  \n",
          "output_type": "stream"
        }
      ],
      "execution_count": null,
      "block_group": "a595443536354e1587499c36e0cd8dab"
    },
    {
      "cell_type": "markdown",
      "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=f64215d6-debc-46bd-b273-63565459a66d' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
      "metadata": {
        "created_in_deepnote_cell": true,
        "deepnote_cell_type": "markdown"
      }
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "deepnote_persisted_session": {
      "createdAt": "2023-12-29T15:45:26.124Z"
    },
    "deepnote_notebook_id": "668d44b62c5e4f4f952579ee125a1372",
    "deepnote_execution_queue": []
  }
}